{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmordig/reinforcement/HumbleAttemptAtGeneralAI/geometry_translation/new/verbalization_venv3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import trl\n",
    "trl.trainer.ConstantLengthDataset\n",
    "from trl.extras.dataset_formatting import conversations_formatting_function\n",
    "\n",
    "from datasets import load_dataset, load_from_disk, DatasetDict\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/home/mmordig/reinforcement/HumbleAttemptAtGeneralAI/runs/verbalization/datasets/alpha_geo_small_processed\"\n",
    "raw_datasets = load_from_disk(dataset_dir)\n",
    "dataset = raw_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "System\n",
      "<</SYS>>\n",
      "\n",
      "Question1 [/INST] Answer1 </s><s>[INST] Question2 [/INST] Answer2 </s>\n",
      "################################################################################\n",
      "<s>[INST] <<SYS>>\n",
      "System\n",
      "<</SYS>>\n",
      "\n",
      "Question1 [/INST] Answer1 </s><s>[INST] Question2 [/INST]\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/76446228/setting-padding-token-as-eos-token-when-using-datacollatorforlanguagemodeling-fr\n",
    "\n",
    "\n",
    "model_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model_name_or_path = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)#, add_eos_token=True)\n",
    "# inputs = \"Hello world\"\n",
    "inputs = [\n",
    "    {\"role\": \"system\", \"content\": \"System\"},\n",
    "    # must be ordered as user-assistant alternating sequence\n",
    "    {\"role\": \"user\", \"content\": \"Question1\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Answer1\"},\n",
    "    {\"role\": \"user\", \"content\": \"Question2\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Answer2\"},\n",
    "]\n",
    "print(tokenizer.apply_chat_template(inputs, tokenize=False, add_generation_prompt=True))\n",
    "print(\"#\"*80)\n",
    "\n",
    "inputs = [\n",
    "    {\"role\": \"system\", \"content\": \"System\"},\n",
    "    # must be ordered as user-assistant alternating sequence\n",
    "    {\"role\": \"user\", \"content\": \"Question1\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Answer1\"},\n",
    "    {\"role\": \"user\", \"content\": \"Question2\"},\n",
    "    # {\"role\": \"assistant\", \"content\": \"Answer2\"},\n",
    "]\n",
    "print(tokenizer.apply_chat_template(inputs, tokenize=False, add_generation_prompt=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> ### Question: Points B, A, C are defined such that triangle ABC is an equilateral triangle. Define points D, F, & E such that E, D, and F is a right angle isosceles triangle with the right angle at D. Point G is defined such that G, A, E are three consecutive vertices of a square. Circle centered at C with radius CE intersects circle centered at G with radius GE at H and E. Points I and J are defined such that line IC and line JC are the two tangents to circle centered at B with radius BF at point I and J respectively.. Define point K such that line DI and line AG are parallel. line JK perpendicular to line DI. line EK perpendicular to line AG. line JK meets line EK at the point K. ### Answer: A B C = ieq_triangle A B C; D E F = risos D E F; G = psquare G A E; H = intersection_cc H C G E; I J = tangent I J C B F; K = intersection_tt K J D I E A G;'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"### Question: Points B, A, C are defined such that triangle ABC is an equilateral triangle. Define points D, F, & E such that E, D, and F is a right angle isosceles triangle with the right angle at D. Point G is defined such that G, A, E are three consecutive vertices of a square. Circle centered at C with radius CE intersects circle centered at G with radius GE at H and E. Points I and J are defined such that line IC and line JC are the two tangents to circle centered at B with radius BF at point I and J respectively.. Define point K such that line DI and line AG are parallel. line JK perpendicular to line DI. line EK perpendicular to line AG. line JK meets line EK at the point K. ### Answer: A B C = ieq_triangle A B C; D E F = risos D E F; G = psquare G A E; H = intersection_cc H C G E; I J = tangent I J C B F; K = intersection_tt K J D I E A G;\"\n",
    "tokenizer.decode(tokenizer(query)[\"input_ids\"], skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁### ▁Question : ▁Point s ▁B , ▁A , ▁C ▁are ▁defined ▁such ▁that ▁triangle ▁ABC ▁is ▁an ▁equ il ater al ▁triangle . ▁Define ▁points ▁D , ▁F , ▁& ▁E ▁such ▁that ▁E , ▁D , ▁and ▁F ▁is ▁a ▁right ▁angle ▁is os cel es ▁triangle ▁with ▁the ▁right ▁angle ▁at ▁D . ▁Point ▁G ▁is ▁defined ▁such ▁that ▁G , ▁A , ▁E ▁are ▁three ▁consecutive ▁vertices ▁of ▁a ▁square . ▁Circle ▁centered ▁at ▁C ▁with ▁radius ▁CE ▁intersect s ▁circle ▁centered ▁at ▁G ▁with ▁radius ▁G E ▁at ▁H ▁and ▁E . ▁Point s ▁I ▁and ▁J ▁are ▁defined ▁such ▁that ▁line ▁IC ▁and ▁line ▁J C ▁are ▁the ▁two ▁tang ents ▁to ▁circle ▁centered ▁at ▁B ▁with ▁radius ▁B F ▁at ▁point ▁I ▁and ▁J ▁respectively .. ▁Define ▁point ▁K ▁such ▁that ▁line ▁DI ▁and ▁line ▁AG ▁are ▁parallel . ▁line ▁J K ▁per pend icular ▁to ▁line ▁DI . ▁line ▁E K ▁per pend icular ▁to ▁line ▁AG . ▁line ▁J K ▁meets ▁line ▁E K ▁at ▁the ▁point ▁K . ▁### ▁Answer : ▁A ▁B ▁C ▁= ▁ie q _ triangle ▁A ▁B ▁C ; ▁D ▁E ▁F ▁= ▁ris os ▁D ▁E ▁F ; ▁G ▁= ▁ps quare ▁G ▁A ▁E ; ▁H ▁= ▁intersection _ cc ▁H ▁C ▁G ▁E ; ▁I ▁J ▁= ▁tang ent ▁I ▁J ▁C ▁B ▁F ; ▁K ▁= ▁intersection _ tt ▁K ▁J ▁D ▁I ▁E ▁A ▁G ;'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(query)\n",
    "\" \".join(tokenizer.tokenize(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ss\n",
      "a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "\n",
    "def print_to_str(x):\n",
    "    temp_out = io.StringIO()\n",
    "    print(x, file=temp_out)\n",
    "    return temp_out.getvalue()\n",
    "print(print_to_str(\"ss\\na\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example format: <s>[INST] <<SYS>>\n",
      "You are an expert in translating geometry problems from natural language into formal language.\n",
      "It is crucial to form syntactically valid statements in the formal language that correspond \n",
      "exactly to its natural language description.\n",
      "You should only output the formal language description of the question, not the solution or anything else.    \n",
      "<</SYS>>\n",
      "\n",
      "C, B, & A are defined such that B, A, and C is a right angle isosceles triangle with the right angle at A. D is defined such that D is a point such that ∠CAB is the same as ∠BAD. [/INST] A B C = risos A B C; D = angle_mirror D C A B </s>\n"
     ]
    }
   ],
   "source": [
    "import os, sys; sys.path.insert(0, os.path.join(os.path.dirname(__vsc_ipynb_file__), \".\")) #todo\n",
    "from question_answer_utils import get_question_answer_to_chat_formatter\n",
    "\n",
    "formatting_func = get_question_answer_to_chat_formatter(tokenizer, text_column=\"text\")\n",
    "print(f\"Example format: {formatting_func(dataset[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geometry_translation.new.question_answer_utils import extract_question_answer\n",
    "\n",
    "\n",
    "\n",
    "text_column = \"text\"\n",
    "print(convert_question_answer_to_chat_format(dataset[0], text_column=text_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs = tokenizer(, return_tensors=\"pt\", add_special_tokens=True)\n",
    "encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(encoded_inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from datasets import load_dataset\n",
    "\n",
    "def wrapping_print(x):\n",
    "    print(textwrap.fill(x))\n",
    "    \n",
    "\n",
    "\n",
    "# dataset_dir = \"/fast/mmordig/general_ai_rl/alphageom_project/alpha_geo_processed\"\n",
    "dataset_dir = \"/home/mmordig/reinforcement/HumbleAttemptAtGeneralAI/geometry_translation/new/runs/verb_dataset\"\n",
    "\n",
    "raw_datasets = load_from_disk(dataset_dir)\n",
    "dataset = raw_datasets[\"test\"][:100]\n",
    "for row in dataset[\"text\"]:\n",
    "    # wrapping_print(row)\n",
    "    wrapping_print(extract_question_prompt(row) + \"END\")\n",
    "    print(\"Answer\")\n",
    "    wrapping_print(extract_answer(row) + \"END\")\n",
    "    break\n",
    "# question = format_question_answer(question, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from peft import AutoPeftModelForCausalLM\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "# from gradio.pipelines import load_from_pipeline\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "# output_dir = sys.argv[1]\n",
    "output_dir = \"runs/trl_sft\"\n",
    "model_name_or_path = get_last_checkpoint(output_dir)\n",
    "logger.info(f\"Loading model from '{model_name_or_path}'\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\", load_in_4bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "logger.info(f\"Loaded model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, num_return_sequences=2, num_beams=4, do_sample=True, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ?pipe\n",
    "\n",
    "row = dataset[\"text\"][0]\n",
    "wrapping_print(extract_question_prompt(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for output in pipe(raw_datasets[\"test\"].to_iterable_dataset()):\n",
    "#     print(output)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from datasets import Dataset\n",
    "\n",
    "def gen_from_iterable_dataset(iterable_ds):\n",
    "    yield from iterable_ds\n",
    "\n",
    "to_dataset = lambda iterable_ds: Dataset.from_generator(partial(gen_from_iterable_dataset, iterable_ds), features=iterable_ds.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"rotten_tomatoes\", split=\"train\", streaming=True)\n",
    "to_dataset(ds.take(2))[:2], to_dataset(ds.take(2))[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/home/mmordig/reinforcement/datasets/alpha_geo_small\"\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_dir=dataset_dir)#, nrows=10)\n",
    "next(iter(dataset[\"train\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def formatting_func(row):\n",
    "#     question = row[\"nl_statement\"]\n",
    "#     answer = row[\"fl_statement\"]\n",
    "#     return {\"text\": f\"### Question: {question} ### Answer: {answer}\"}\n",
    "# dataset = dataset.map(formatting_func)\n",
    "\n",
    "def formatting_func_batched(batch):\n",
    "    questions = batch[\"nl_statement\"]\n",
    "    answers = batch[\"fl_statement\"]\n",
    "    return {\"text\": [f\"### Question: {question} ### Answer: {answer}\" for (question, answer) in zip(questions, answers)]}\n",
    "\n",
    "dataset = dataset.map(formatting_func_batched, batched=True)\n",
    "dataset = dataset.remove_columns(set(dataset[\"train\"].column_names) - {\"text\"})\n",
    "dataset = dataset[\"train\"].train_test_split(0.1, seed=1)\n",
    "dataset_filename = \"/home/mmordig/reinforcement/HumbleAttemptAtGeneralAI/geometry_translation/new/runs/verb_dataset\"\n",
    "dataset.save_to_disk(dataset_filename)\n",
    "dataset, dataset[\"train\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset(dataset_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "dataset_dir = \"/home/mmordig/reinforcement/datasets/alpha_geo_small\"\n",
    "model_name_or_path = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side=\"left\")\n",
    "\n",
    "tokenizer(\" \")[\"input_ids\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_constant_length_dataset(dataset_dir, **kwargs):\n",
    "    # use with SFTTrainer together with arg packing=True, not compatible with DataCollatorForCompletionOnlyLM\n",
    "    \n",
    "    text_dataset = load_dataset(\"csv\", data_dir=dataset_dir, **kwargs)\n",
    "    # next(iter(text_dataset[\"train\"]))\n",
    "    \n",
    "    def formatting_func(row):\n",
    "        question = row[\"nl_statement\"]\n",
    "        answer = row[\"fl_statement\"]\n",
    "        return f\"### Question: {question} ### Answer: {answer}\"\n",
    "\n",
    "    dataset = trl.trainer.ConstantLengthDataset(tokenizer, text_dataset[\"train\"], formatting_func=formatting_func)\n",
    "    # return dataset\n",
    "    return DatasetDict({\n",
    "        \"train\": dataset,\n",
    "    })\n",
    "dataset = get_constant_length_dataset(dataset_dir)\n",
    "res = next(iter(dataset[\"train\"]))\n",
    "tokenizer.decode(res[\"input_ids\"]).split(\"### \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
