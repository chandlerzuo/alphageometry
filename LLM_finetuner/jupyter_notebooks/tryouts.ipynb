{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmordig/reinforcement/alphageometry/LLM_finetuner/verbalization_venv3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    }
   ],
   "source": [
    "import trl\n",
    "trl.trainer.ConstantLengthDataset\n",
    "from trl.extras.dataset_formatting import conversations_formatting_function\n",
    "\n",
    "from datasets import load_dataset, load_from_disk, DatasetDict\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmordig/reinforcement/alphageometry/LLM_finetuner/verbalization_venv3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_dir = \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/runs/verbalization/datasets/alpha_geo_small_arrow\"\n",
    "dataset = load_dataset(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['nl_statement', 'fl_statement'],\n",
       "        num_rows: 355\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['nl_statement', 'fl_statement'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['nl_statement', 'fl_statement'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset\n",
    "dataset[\"train\"][0]\n",
    "dataset[\"train\"][:2]\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eqdia_quadrangle', 'intersection_cc', 'intersection_lc', 'intersection_ll', 'intersection_lp', 'intersection_lt', 'intersection_pp', 'intersection_tt', 'angle_bisector', 'eq_quadrangle', 'parallelogram', 'angle_mirror', 'circumcenter', 'eq_trapezoid', 'iso_triangle', 'ieq_triangle', 'eq_triangle', 'orthocenter', 'r_trapezoid', 'cc_tangent0', 'eqdistance', 'ninepoints', 'lc_tangent', 'quadrangle', 'r_triangle', 'triangle12', 'trisegment', 'cc_tangent', 'incenter2', 'excenter2', 'on_aline2', 'on_circle', 'rectangle', 'trapezoid', 'on_opline', 'on_circum', 'eqangle2', 'incenter', 'excenter', 'centroid', 'midpoint', 'on_aline', 'on_bline', 'on_pline', 'on_tline', 'pentagon', 'triangle', 'eqangle3', 'nsquare', 'on_line', 'psquare', 'reflect', 's_angle', 'segment', 'isquare', 'trisect', 'tangent', 'circle', 'mirror', 'square', 'on_dia', 'risos', 'shift', 'e5128', 'free', 'foot', '2l1c', '3peq']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'nl_statement': ['Points B, A, C are defined such that triangle ABC is an equilateral triangle. Define points D, F, & E such that E, D, and F is a right angle isosceles triangle with the right angle at D. Point G is defined such that G, A, E are three consecutive vertices of a square. Circle centered at C with radius CE intersects circle centered at G with radius GE at H and E. Points I and J are defined such that line IC and line JC are the two tangents to circle centered at B with radius BF at point I and J respectively.. Define point K such that line DI and line AG are parallel. line JK perpendicular to line DI. line EK perpendicular to line AG. line JK meets line EK at the point K.',\n",
       "  'Points A, D, B, C are defined such that A, B, C, and D is a trapezoid where line AD is equal to line BC. E is a point. F is defined such that line FC is touching to circle centered at A with radius AC at the point F. Define points G, J, I, & H such that J is located at the incenter of triangle ECB with touchpoints H, I, and G. K is defined such that K is a point such that ∠FDJ = ∠JDK.'],\n",
       " 'fl_statement': ['A B C = [ieq_triangle] A B C; D E F = [risos] D E F; G = [psquare] G A E; H = [intersection_cc] H C G E; I J = [tangent] I J C B F; K = [intersection_tt] K J D I E A G',\n",
       "  'A B C D = [eq_trapezoid] A B C D; E = [free] E; F = [lc_tangent] F C A; G H I J = [incenter2] G H I J E C B; K = [angle_mirror] K F D J']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from LLM_finetuner.dataset import replace_geometry_words_with_tokens, format_question_answer_single\n",
    "\n",
    "\n",
    "extra_tokens_file = Path(os.path.expanduser(\"~/reinforcement/alphageometry/assets/def-patterns-desc.yml\"))\n",
    "def_to_desc = yaml.safe_load(extra_tokens_file.read_text())\n",
    "# eq_triangle, ieq_triangle, so replace by longest first\n",
    "geom_tokens = sorted(list(def_to_desc.keys()), key=lambda x: -len(x))\n",
    "print(geom_tokens)\n",
    "\n",
    "dataset = dataset.map(lambda x: {\"fl_statement\": replace_geometry_words_with_tokens(x[\"fl_statement\"], geom_tokens)}, num_proc=8)\n",
    "dataset[\"train\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mencoded_inputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchEncoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchEncoding\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpadding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPaddingStrategy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpad_to_multiple_of\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_attention_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchEncoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Pad a single encoded input or a batch of encoded inputs up to predefined length or to the max sequence length\n",
      "in the batch.\n",
      "\n",
      "Padding side (left/right) padding token ids are defined at the tokenizer level (with `self.padding_side`,\n",
      "`self.pad_token_id` and `self.pad_token_type_id`).\n",
      "\n",
      "Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the\n",
      "text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "<Tip>\n",
      "\n",
      "If the `encoded_inputs` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the\n",
      "result will use the same type unless you provide a different tensor type with `return_tensors`. In the case of\n",
      "PyTorch tensors, you will lose the specific device of your tensors however.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "Args:\n",
      "    encoded_inputs ([`BatchEncoding`], list of [`BatchEncoding`], `Dict[str, List[int]]`, `Dict[str, List[List[int]]` or `List[Dict[str, List[int]]]`):\n",
      "        Tokenized inputs. Can represent one input ([`BatchEncoding`] or `Dict[str, List[int]]`) or a batch of\n",
      "        tokenized inputs (list of [`BatchEncoding`], *Dict[str, List[List[int]]]* or *List[Dict[str,\n",
      "        List[int]]]*) so you can use this method during preprocessing as well as in a PyTorch Dataloader\n",
      "        collate function.\n",
      "\n",
      "        Instead of `List[int]` you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors), see\n",
      "        the note above for the return type.\n",
      "    padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n",
      "         Select a strategy to pad the returned sequences (according to the model's padding side and padding\n",
      "         index) among:\n",
      "\n",
      "        - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      "          sequence if provided).\n",
      "        - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      "          acceptable input length for the model if that argument is not provided.\n",
      "        - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      "          lengths).\n",
      "    max_length (`int`, *optional*):\n",
      "        Maximum length of the returned list and optionally padding length (see above).\n",
      "    pad_to_multiple_of (`int`, *optional*):\n",
      "        If set will pad the sequence to a multiple of the provided value.\n",
      "\n",
      "        This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      "        `>= 7.5` (Volta).\n",
      "    return_attention_mask (`bool`, *optional*):\n",
      "        Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      "        to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      "\n",
      "        [What are attention masks?](../glossary#attention-mask)\n",
      "    return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      "        If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      "\n",
      "        - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      "        - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      "        - `'np'`: Return Numpy `np.ndarray` objects.\n",
      "    verbose (`bool`, *optional*, defaults to `True`):\n",
      "        Whether or not to print more information and warnings.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/reinforcement/alphageometry/LLM_finetuner/verbalization_venv3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "# from transformers import DataCollatorWithPadding\n",
    "?tokenizer.pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see transformers.Trainer.evaluation_loop\n",
    "# from transformers import Trainer, TrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "import sys\n",
    "from LLM_finetuner.sft_finetuning import main as get_trainer\n",
    "import LLM_finetuner\n",
    "LLM_finetuner.sft_finetuning.TRL_USE_RICH = False\n",
    "\n",
    "sys.argv = [\"--overwrite_output_dir\", \"--use_peft\", \"--per_device_train_batch_size\", \"64\", \"--per_device_eval_batch_size\", \"64\", \"--model_name_or_path\", \"gpt2\", \"--eval_steps\", \"10\", \"--evaluation_strategy\", \"steps\", \"--max_eval_samples\", \"2\", \"--explicit_eos_str\", \"[END]\", \"--extra_tokens_file\", \"/home/mmordig/reinforcement/alphageometry/assets/def-patterns-desc.yml\", \"--output_dir\", \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/runs/verbalization/training/debug112/{model_name}_{max_train_samples}ex_peft{use_peft}\", \"--dataset_name\", \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/runs/verbalization/datasets/alpha_geo_processed\", \"--config\", \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/trl_sft_config.yml\", \"--max_train_samples\", \"10\", \"--num_train_epochs\", \"100000\",]\n",
    "sys.argv += [\"--include_inputs_for_metrics\"]\n",
    "# sys.argv += [\"--dataloader_num_workers: 0\"]\n",
    "trainer, resume_checkpoint = get_trainer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "def compute_metrics(eval_results):\n",
    "    breakpoint()\n",
    "    predictions = eval_results.predictions\n",
    "    inputs = eval_results.inputs\n",
    "    labels = eval_results.label_ids\n",
    "    # print(predictions)\n",
    "    print(inputs[0])\n",
    "    print(labels[0])\n",
    "    print(predictions[0])\n",
    "    predictions = trainer.tokenizer.batch_decode(predictions)\n",
    "    inputs = trainer.tokenizer.batch_decode(inputs)\n",
    "    labels = trainer.tokenizer.batch_decode(labels)\n",
    "    print(predictions)\n",
    "    print(labels)\n",
    "    # print(predictions.shape)\n",
    "    # return {\"generated_text\": predictions}\n",
    "    # return {\"generated_text2\": wandb.Table(columns=[\"text\"], data=[[p] for p in predictions])}\n",
    "    return {\"predictions\": wandb.Table(columns=[\"input\", \"predicted\"], data=[[i, p] for (i, p) in zip(inputs, predictions)])}\n",
    "    \n",
    "trainer.compute_metrics = compute_metrics\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TrainerWithGeneration(Trainer):\n",
    "#     def generate_with_model(self):\n",
    "#         model = self._wrap_model(self.model, training=False, dataloader=dataloader)\n",
    "#         model.eval()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class MakePredictionsCallback(TrainerCallback):\n",
    "    def __init__(self, max_generation_length=1024):\n",
    "        # self.tokenizer = tokenizer\n",
    "        self.max_generation_length = max_generation_length\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, model, eval_dataloader, **kwargs):\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        for batch in eval_dataloader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            with torch.no_grad():\n",
    "                output = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=self.max_generation_length)\n",
    "            predictions.extend(output)\n",
    "        # model.train()\n",
    "        \n",
    "        control.on_lo\n",
    "        return control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from contextlib import nullcontext\n",
    "\n",
    "# from torchdistx.fake import fake_mode\n",
    "# with fake_mode():\n",
    "with nullcontext():\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1, 20, 5),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(20, 64, 5),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "    total_activations = 0\n",
    "\n",
    "    def count_activations_hook(module, input, output):\n",
    "        global total_activations\n",
    "        activations = output.numel()\n",
    "        total_activations += activations\n",
    "        print(f\"{module.__class__.__name__} produced {activations} activations\")\n",
    "\n",
    "    for layer in model.children():\n",
    "        layer.register_forward_hook(count_activations_hook)\n",
    "\n",
    "    input_tensor = torch.randn(1, 1, 28, 28)\n",
    "    model(input_tensor)\n",
    "    print(f\"Total activations: {total_activations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "from accelerate import init_empty_weights\n",
    "model_name = \"gpt2\"\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model(input_ids=torch.LongTensor([1, 2, 3])[None, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(model)\n",
    "# model.modules[0]\n",
    "next(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_activations = 0\n",
    "\n",
    "def count_activations_hook(module, input, output):\n",
    "    global total_activations\n",
    "    # output.shape[0] is the batch size\n",
    "    # output.numel() gives the total number of elements in the output, for all batches\n",
    "    # To get activations per single input, you would divide by output.shape[0]\n",
    "    activations = output.numel()\n",
    "    total_activations += activations\n",
    "    print(f\"{module.__class__.__name__} produced {activations} activations\")\n",
    "\n",
    "# Register hook for each layer\n",
    "for layer in model.children():\n",
    "    layer.register_forward_hook(count_activations_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "print(accelerator.state)\n",
    "print(str(accelerator))\n",
    "print(f\"{accelerator=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "    \n",
    "# r'''one \\\"two\\\" 'three''''\n",
    "# parse_htcondor_arguments('''\"3 simple arguments\"''')\n",
    "\n",
    "from accelerate import init_empty_weights\n",
    "\n",
    "\n",
    "# format_htcondor_arguments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate\n",
    "accelerate.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple(\"$(Cluster) $(Process) $(NumRanks)\".split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LLM_finetuner.utils import add_new_tokens_with_average_init\n",
    "\n",
    "\n",
    "extra_tokens_file = Path(os.path.expanduser(\"~/reinforcement/alphageometry/assets/def-patterns-desc.yml\"))\n",
    "def_to_desc = yaml.safe_load(extra_tokens_file.read_text())\n",
    "\n",
    "add_new_tokens_with_average_init(model, tokenizer, def_to_desc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def_to_desc\n",
    "tokens = list(def_to_desc.keys())\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"The following tokens are already known, ignoring them: {[token for (token, id) in zip(tokens, token_ids) if id != tokenizer.unk_token_id]}\")\n",
    "def_to_desc = {token: def_to_desc[token] for (token, id) in zip(tokens, token_ids) if id == tokenizer.unk_token_id}\n",
    "def_to_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/home/mmordig/reinforcement/HumbleAttemptAtGeneralAI/runs/verbalization/datasets/alpha_geo_small_processed\"\n",
    "raw_datasets = load_from_disk(dataset_dir)\n",
    "dataset = raw_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/76446228/setting-padding-token-as-eos-token-when-using-datacollatorforlanguagemodeling-fr\n",
    "\n",
    "\n",
    "model_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model_name_or_path = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)#, add_eos_token=True)\n",
    "# inputs = \"Hello world\"\n",
    "inputs = [\n",
    "    {\"role\": \"system\", \"content\": \"System\"},\n",
    "    # must be ordered as user-assistant alternating sequence\n",
    "    {\"role\": \"user\", \"content\": \"Question1\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Answer1\"},\n",
    "    {\"role\": \"user\", \"content\": \"Question2\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Answer2\"},\n",
    "]\n",
    "print(tokenizer.apply_chat_template(inputs, tokenize=False, add_generation_prompt=True))\n",
    "print(\"#\"*80)\n",
    "\n",
    "inputs = [\n",
    "    {\"role\": \"system\", \"content\": \"System\"},\n",
    "    # must be ordered as user-assistant alternating sequence\n",
    "    {\"role\": \"user\", \"content\": \"Question1\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Answer1\"},\n",
    "    {\"role\": \"user\", \"content\": \"Question2\"},\n",
    "    # {\"role\": \"assistant\", \"content\": \"Answer2\"},\n",
    "]\n",
    "print(tokenizer.apply_chat_template(inputs, tokenize=False, add_generation_prompt=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"### Question: Points B, A, C are defined such that triangle ABC is an equilateral triangle. Define points D, F, & E such that E, D, and F is a right angle isosceles triangle with the right angle at D. Point G is defined such that G, A, E are three consecutive vertices of a square. Circle centered at C with radius CE intersects circle centered at G with radius GE at H and E. Points I and J are defined such that line IC and line JC are the two tangents to circle centered at B with radius BF at point I and J respectively.. Define point K such that line DI and line AG are parallel. line JK perpendicular to line DI. line EK perpendicular to line AG. line JK meets line EK at the point K. ### Answer: A B C = ieq_triangle A B C; D E F = risos D E F; G = psquare G A E; H = intersection_cc H C G E; I J = tangent I J C B F; K = intersection_tt K J D I E A G;\"\n",
    "tokenizer.decode(tokenizer(query)[\"input_ids\"], skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(query)\n",
    "\" \".join(tokenizer.tokenize(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "\n",
    "def print_to_str(x):\n",
    "    temp_out = io.StringIO()\n",
    "    print(x, file=temp_out)\n",
    "    return temp_out.getvalue()\n",
    "print(print_to_str(\"ss\\na\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LLM_finetuner.question_answer_utils import get_question_answer_to_chat_formatter\n",
    "\n",
    "formatting_func = get_question_answer_to_chat_formatter(tokenizer, text_column=\"text\")\n",
    "print(f\"Example format: {formatting_func(dataset[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geometry_translation.new.question_answer_utils import extract_question_answer\n",
    "\n",
    "\n",
    "\n",
    "text_column = \"text\"\n",
    "print(convert_question_answer_to_chat_format(dataset[0], text_column=text_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs = tokenizer(, return_tensors=\"pt\", add_special_tokens=True)\n",
    "encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(encoded_inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from datasets import load_dataset\n",
    "\n",
    "def wrapping_print(x):\n",
    "    print(textwrap.fill(x))\n",
    "    \n",
    "\n",
    "\n",
    "# dataset_dir = \"/fast/mmordig/general_ai_rl/alphageom_project/alpha_geo_processed\"\n",
    "dataset_dir = \"/home/mmordig/reinforcement/HumbleAttemptAtGeneralAI/geometry_translation/new/runs/verb_dataset\"\n",
    "\n",
    "raw_datasets = load_from_disk(dataset_dir)\n",
    "dataset = raw_datasets[\"test\"][:100]\n",
    "for row in dataset[\"text\"]:\n",
    "    # wrapping_print(row)\n",
    "    wrapping_print(extract_question_prompt(row) + \"END\")\n",
    "    print(\"Answer\")\n",
    "    wrapping_print(extract_answer(row) + \"END\")\n",
    "    break\n",
    "# question = format_question_answer(question, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from peft import AutoPeftModelForCausalLM\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "# from gradio.pipelines import load_from_pipeline\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "# output_dir = sys.argv[1]\n",
    "output_dir = \"runs/trl_sft\"\n",
    "model_name_or_path = get_last_checkpoint(output_dir)\n",
    "logger.info(f\"Loading model from '{model_name_or_path}'\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\", load_in_4bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "logger.info(f\"Loaded model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, num_return_sequences=2, num_beams=4, do_sample=True, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ?pipe\n",
    "\n",
    "row = dataset[\"text\"][0]\n",
    "wrapping_print(extract_question_prompt(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for output in pipe(raw_datasets[\"test\"].to_iterable_dataset()):\n",
    "#     print(output)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from datasets import Dataset\n",
    "\n",
    "def gen_from_iterable_dataset(iterable_ds):\n",
    "    yield from iterable_ds\n",
    "\n",
    "to_dataset = lambda iterable_ds: Dataset.from_generator(partial(gen_from_iterable_dataset, iterable_ds), features=iterable_ds.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"rotten_tomatoes\", split=\"train\", streaming=True)\n",
    "to_dataset(ds.take(2))[:2], to_dataset(ds.take(2))[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/home/mmordig/reinforcement/datasets/alpha_geo_small\"\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_dir=dataset_dir)#, nrows=10)\n",
    "next(iter(dataset[\"train\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def formatting_func(row):\n",
    "#     question = row[\"nl_statement\"]\n",
    "#     answer = row[\"fl_statement\"]\n",
    "#     return {\"text\": f\"### Question: {question} ### Answer: {answer}\"}\n",
    "# dataset = dataset.map(formatting_func)\n",
    "\n",
    "def formatting_func_batched(batch):\n",
    "    questions = batch[\"nl_statement\"]\n",
    "    answers = batch[\"fl_statement\"]\n",
    "    return {\"text\": [f\"### Question: {question} ### Answer: {answer}\" for (question, answer) in zip(questions, answers)]}\n",
    "\n",
    "dataset = dataset.map(formatting_func_batched, batched=True)\n",
    "dataset = dataset.remove_columns(set(dataset[\"train\"].column_names) - {\"text\"})\n",
    "dataset = dataset[\"train\"].train_test_split(0.1, seed=1)\n",
    "dataset_filename = \"/home/mmordig/reinforcement/HumbleAttemptAtGeneralAI/geometry_translation/new/runs/verb_dataset\"\n",
    "dataset.save_to_disk(dataset_filename)\n",
    "dataset, dataset[\"train\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset(dataset_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "dataset_dir = \"/home/mmordig/reinforcement/datasets/alpha_geo_small\"\n",
    "model_name_or_path = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side=\"left\")\n",
    "\n",
    "tokenizer(\" \")[\"input_ids\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_constant_length_dataset(dataset_dir, **kwargs):\n",
    "    # use with SFTTrainer together with arg packing=True, not compatible with DataCollatorForCompletionOnlyLM\n",
    "    \n",
    "    text_dataset = load_dataset(\"csv\", data_dir=dataset_dir, **kwargs)\n",
    "    # next(iter(text_dataset[\"train\"]))\n",
    "    \n",
    "    def formatting_func(row):\n",
    "        question = row[\"nl_statement\"]\n",
    "        answer = row[\"fl_statement\"]\n",
    "        return f\"### Question: {question} ### Answer: {answer}\"\n",
    "\n",
    "    dataset = trl.trainer.ConstantLengthDataset(tokenizer, text_dataset[\"train\"], formatting_func=formatting_func)\n",
    "    # return dataset\n",
    "    return DatasetDict({\n",
    "        \"train\": dataset,\n",
    "    })\n",
    "dataset = get_constant_length_dataset(dataset_dir)\n",
    "res = next(iter(dataset[\"train\"]))\n",
    "tokenizer.decode(res[\"input_ids\"]).split(\"### \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
