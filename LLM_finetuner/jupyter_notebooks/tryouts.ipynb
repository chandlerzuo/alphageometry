{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmordig/reinforcement/alphageometry/LLM_finetuner/verbalization_venv3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    }
   ],
   "source": [
    "import trl\n",
    "trl.trainer.ConstantLengthDataset\n",
    "from trl.extras.dataset_formatting import conversations_formatting_function\n",
    "\n",
    "from datasets import load_dataset, load_from_disk, DatasetDict\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class MakePredictionsCallback(TrainerCallback):\n",
    "    def __init__(self, max_generation_length=1024):\n",
    "        # self.tokenizer = tokenizer\n",
    "        self.max_generation_length = max_generation_length\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, model, eval_dataloader, **kwargs):\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        for batch in eval_dataloader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            with torch.no_grad():\n",
    "                output = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=self.max_generation_length)\n",
    "            predictions.extend(output)\n",
    "        # model.train()\n",
    "        \n",
    "        control.on_lo\n",
    "        return control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "convolution_overrideable not implemented. You are likely triggering this with tensor backend other than CPU/CUDA/MKLDNN, if this is intended, please use TORCH_LIBRARY_IMPL to override this function ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Create a tensor on the 'meta' device\u001b[39;00m\n\u001b[1;32m     29\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal activations: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_activations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/reinforcement/alphageometry/LLM_finetuner/verbalization_venv3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move and/or cast the parameters and buffers.\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \n\u001b[1;32m   1054\u001b[0m \u001b[38;5;124;03m    This can be called as\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \n\u001b[1;32m   1056\u001b[0m \u001b[38;5;124;03m    .. function:: to(device=None, dtype=None, non_blocking=False)\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;124;03m       :noindex:\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \n\u001b[1;32m   1059\u001b[0m \u001b[38;5;124;03m    .. function:: to(dtype, non_blocking=False)\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;124;03m       :noindex:\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \n\u001b[1;32m   1062\u001b[0m \u001b[38;5;124;03m    .. function:: to(tensor, non_blocking=False)\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;124;03m       :noindex:\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \n\u001b[1;32m   1065\u001b[0m \u001b[38;5;124;03m    .. function:: to(memory_format=torch.channels_last)\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;124;03m       :noindex:\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \n\u001b[1;32m   1068\u001b[0m \u001b[38;5;124;03m    Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m    floating point or complex :attr:`dtype`\\ s. In addition, this method will\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    only cast the floating point or complex parameters and buffers to :attr:`dtype`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m    (if given). The integral parameters and buffers will be moved\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;124;03m    :attr:`device`, if that is given, but with dtypes unchanged. When\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;124;03m    :attr:`non_blocking` is set, it tries to convert/move asynchronously\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;124;03m    with respect to the host if possible, e.g., moving CPU Tensors with\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;124;03m    pinned memory to CUDA devices.\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m \n\u001b[1;32m   1077\u001b[0m \u001b[38;5;124;03m    See below for examples.\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m \n\u001b[1;32m   1079\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;124;03m        This method modifies the module in-place.\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \n\u001b[1;32m   1082\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;124;03m        device (:class:`torch.device`): the desired device of the parameters\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;124;03m            and buffers in this module\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;124;03m        dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;124;03m            the parameters and buffers in this module\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;124;03m        tensor (torch.Tensor): Tensor whose dtype and device are the desired\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;124;03m            dtype and device for all parameters and buffers in this module\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;124;03m        memory_format (:class:`torch.memory_format`): the desired memory\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;124;03m            format for 4D parameters and buffers in this module (keyword\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;124;03m            only argument)\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \n\u001b[1;32m   1093\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \n\u001b[1;32m   1096\u001b[0m \u001b[38;5;124;03m    Examples::\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \n\u001b[1;32m   1098\u001b[0m \u001b[38;5;124;03m        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;124;03m        >>> linear = nn.Linear(2, 2)\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;124;03m        >>> linear.weight\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;124;03m        Parameter containing:\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;124;03m        tensor([[ 0.1913, -0.3420],\u001b[39;00m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;124;03m                [-0.5113, -0.2325]])\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;124;03m        >>> linear.to(torch.double)\u001b[39;00m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;124;03m        Linear(in_features=2, out_features=2, bias=True)\u001b[39;00m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;124;03m        >>> linear.weight\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;124;03m        Parameter containing:\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;124;03m        tensor([[ 0.1913, -0.3420],\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;124;03m                [-0.5113, -0.2325]], dtype=torch.float64)\u001b[39;00m\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;124;03m        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\u001b[39;00m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;124;03m        >>> gpu1 = torch.device(\"cuda:1\")\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;124;03m        >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;124;03m        Linear(in_features=2, out_features=2, bias=True)\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;124;03m        >>> linear.weight\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;124;03m        Parameter containing:\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;124;03m        tensor([[ 0.1914, -0.3420],\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;124;03m                [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;124;03m        >>> cpu = torch.device(\"cpu\")\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;124;03m        >>> linear.to(cpu)\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;124;03m        Linear(in_features=2, out_features=2, bias=True)\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;124;03m        >>> linear.weight\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;124;03m        Parameter containing:\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;124;03m        tensor([[ 0.1914, -0.3420],\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;124;03m                [-0.5112, -0.2324]], dtype=torch.float16)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \n\u001b[1;32m   1126\u001b[0m \u001b[38;5;124;03m        >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;124;03m        >>> linear.weight\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;124;03m        Parameter containing:\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;124;03m        tensor([[ 0.3741+0.j,  0.2382+0.j],\u001b[39;00m\n\u001b[0;32m-> 1130\u001b[0m \u001b[38;5;124;03m                [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;124;03m        >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;124;03m        tensor([[0.6122+0.j, 0.1150+0.j],\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;124;03m                [0.6122+0.j, 0.1150+0.j],\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;124;03m                [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \n\u001b[1;32m   1136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m     device, dtype, non_blocking, convert_to_format \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_parse_to(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/reinforcement/alphageometry/LLM_finetuner/verbalization_venv3/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    136\u001b[0m     str_indices \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules))]\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules \u001b[38;5;241m=\u001b[39m OrderedDict(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(str_indices, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mvalues())))\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;129m@_copy_to_script_wrapper\u001b[39m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__add__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSequential\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/reinforcement/alphageometry/LLM_finetuner/verbalization_venv3/lib/python3.10/site-packages/torch/nn/modules/module.py:1148\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnn.Module.to only accepts floating point or complex \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1142\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtypes, but got desired dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mis_complex:\n\u001b[1;32m   1144\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1145\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex modules are a new feature under active development whose design may change, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1146\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand some modules might not work as expected when using complex tensors as parameters or buffers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1147\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.yml \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1148\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif a complex module does not work as expected.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(t):\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/reinforcement/alphageometry/LLM_finetuner/verbalization_venv3/lib/python3.10/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m--> 457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "File \u001b[0;32m~/reinforcement/alphageometry/LLM_finetuner/verbalization_venv3/lib/python3.10/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_conv_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, weight: Tensor, bias: Optional[Tensor]):\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 453\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                         weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                         _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    457\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: convolution_overrideable not implemented. You are likely triggering this with tensor backend other than CPU/CUDA/MKLDNN, if this is intended, please use TORCH_LIBRARY_IMPL to override this function "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from contextlib import nullcontext\n",
    "\n",
    "# from torchdistx.fake import fake_mode\n",
    "# with fake_mode():\n",
    "with nullcontext():\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1, 20, 5),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(20, 64, 5),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "    total_activations = 0\n",
    "\n",
    "    def count_activations_hook(module, input, output):\n",
    "        global total_activations\n",
    "        activations = output.numel()\n",
    "        total_activations += activations\n",
    "        print(f\"{module.__class__.__name__} produced {activations} activations\")\n",
    "\n",
    "    for layer in model.children():\n",
    "        layer.register_forward_hook(count_activations_hook)\n",
    "\n",
    "    input_tensor = torch.randn(1, 1, 28, 28)\n",
    "    model(input_tensor)\n",
    "    print(f\"Total activations: {total_activations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "from accelerate import init_empty_weights\n",
    "model_name = \"gpt2\"\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model(input_ids=torch.LongTensor([1, 2, 3])[None, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(model)\n",
    "# model.modules[0]\n",
    "next(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_activations = 0\n",
    "\n",
    "def count_activations_hook(module, input, output):\n",
    "    global total_activations\n",
    "    # output.shape[0] is the batch size\n",
    "    # output.numel() gives the total number of elements in the output, for all batches\n",
    "    # To get activations per single input, you would divide by output.shape[0]\n",
    "    activations = output.numel()\n",
    "    total_activations += activations\n",
    "    print(f\"{module.__class__.__name__} produced {activations} activations\")\n",
    "\n",
    "# Register hook for each layer\n",
    "for layer in model.children():\n",
    "    layer.register_forward_hook(count_activations_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "print(accelerator.state)\n",
    "print(str(accelerator))\n",
    "print(f\"{accelerator=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "    \n",
    "# r'''one \\\"two\\\" 'three''''\n",
    "# parse_htcondor_arguments('''\"3 simple arguments\"''')\n",
    "\n",
    "from accelerate import init_empty_weights\n",
    "\n",
    "\n",
    "# format_htcondor_arguments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate\n",
    "accelerate.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple(\"$(Cluster) $(Process) $(NumRanks)\".split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LLM_finetuner.utils import add_new_tokens_with_average_init\n",
    "\n",
    "\n",
    "extra_tokens_file = Path(os.path.expanduser(\"~/reinforcement/alphageometry/assets/def-patterns-desc.yml\"))\n",
    "def_to_desc = yaml.safe_load(extra_tokens_file.read_text())\n",
    "\n",
    "add_new_tokens_with_average_init(model, tokenizer, def_to_desc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def_to_desc\n",
    "tokens = list(def_to_desc.keys())\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"The following tokens are already known, ignoring them: {[token for (token, id) in zip(tokens, token_ids) if id != tokenizer.unk_token_id]}\")\n",
    "def_to_desc = {token: def_to_desc[token] for (token, id) in zip(tokens, token_ids) if id == tokenizer.unk_token_id}\n",
    "def_to_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/home/mmordig/reinforcement/HumbleAttemptAtGeneralAI/runs/verbalization/datasets/alpha_geo_small_processed\"\n",
    "raw_datasets = load_from_disk(dataset_dir)\n",
    "dataset = raw_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/76446228/setting-padding-token-as-eos-token-when-using-datacollatorforlanguagemodeling-fr\n",
    "\n",
    "\n",
    "model_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model_name_or_path = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)#, add_eos_token=True)\n",
    "# inputs = \"Hello world\"\n",
    "inputs = [\n",
    "    {\"role\": \"system\", \"content\": \"System\"},\n",
    "    # must be ordered as user-assistant alternating sequence\n",
    "    {\"role\": \"user\", \"content\": \"Question1\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Answer1\"},\n",
    "    {\"role\": \"user\", \"content\": \"Question2\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Answer2\"},\n",
    "]\n",
    "print(tokenizer.apply_chat_template(inputs, tokenize=False, add_generation_prompt=True))\n",
    "print(\"#\"*80)\n",
    "\n",
    "inputs = [\n",
    "    {\"role\": \"system\", \"content\": \"System\"},\n",
    "    # must be ordered as user-assistant alternating sequence\n",
    "    {\"role\": \"user\", \"content\": \"Question1\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Answer1\"},\n",
    "    {\"role\": \"user\", \"content\": \"Question2\"},\n",
    "    # {\"role\": \"assistant\", \"content\": \"Answer2\"},\n",
    "]\n",
    "print(tokenizer.apply_chat_template(inputs, tokenize=False, add_generation_prompt=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"### Question: Points B, A, C are defined such that triangle ABC is an equilateral triangle. Define points D, F, & E such that E, D, and F is a right angle isosceles triangle with the right angle at D. Point G is defined such that G, A, E are three consecutive vertices of a square. Circle centered at C with radius CE intersects circle centered at G with radius GE at H and E. Points I and J are defined such that line IC and line JC are the two tangents to circle centered at B with radius BF at point I and J respectively.. Define point K such that line DI and line AG are parallel. line JK perpendicular to line DI. line EK perpendicular to line AG. line JK meets line EK at the point K. ### Answer: A B C = ieq_triangle A B C; D E F = risos D E F; G = psquare G A E; H = intersection_cc H C G E; I J = tangent I J C B F; K = intersection_tt K J D I E A G;\"\n",
    "tokenizer.decode(tokenizer(query)[\"input_ids\"], skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(query)\n",
    "\" \".join(tokenizer.tokenize(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "\n",
    "def print_to_str(x):\n",
    "    temp_out = io.StringIO()\n",
    "    print(x, file=temp_out)\n",
    "    return temp_out.getvalue()\n",
    "print(print_to_str(\"ss\\na\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LLM_finetuner.question_answer_utils import get_question_answer_to_chat_formatter\n",
    "\n",
    "formatting_func = get_question_answer_to_chat_formatter(tokenizer, text_column=\"text\")\n",
    "print(f\"Example format: {formatting_func(dataset[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geometry_translation.new.question_answer_utils import extract_question_answer\n",
    "\n",
    "\n",
    "\n",
    "text_column = \"text\"\n",
    "print(convert_question_answer_to_chat_format(dataset[0], text_column=text_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs = tokenizer(, return_tensors=\"pt\", add_special_tokens=True)\n",
    "encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(encoded_inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from datasets import load_dataset\n",
    "\n",
    "def wrapping_print(x):\n",
    "    print(textwrap.fill(x))\n",
    "    \n",
    "\n",
    "\n",
    "# dataset_dir = \"/fast/mmordig/general_ai_rl/alphageom_project/alpha_geo_processed\"\n",
    "dataset_dir = \"/home/mmordig/reinforcement/HumbleAttemptAtGeneralAI/geometry_translation/new/runs/verb_dataset\"\n",
    "\n",
    "raw_datasets = load_from_disk(dataset_dir)\n",
    "dataset = raw_datasets[\"test\"][:100]\n",
    "for row in dataset[\"text\"]:\n",
    "    # wrapping_print(row)\n",
    "    wrapping_print(extract_question_prompt(row) + \"END\")\n",
    "    print(\"Answer\")\n",
    "    wrapping_print(extract_answer(row) + \"END\")\n",
    "    break\n",
    "# question = format_question_answer(question, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from peft import AutoPeftModelForCausalLM\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "# from gradio.pipelines import load_from_pipeline\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "# output_dir = sys.argv[1]\n",
    "output_dir = \"runs/trl_sft\"\n",
    "model_name_or_path = get_last_checkpoint(output_dir)\n",
    "logger.info(f\"Loading model from '{model_name_or_path}'\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\", load_in_4bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "logger.info(f\"Loaded model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, num_return_sequences=2, num_beams=4, do_sample=True, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ?pipe\n",
    "\n",
    "row = dataset[\"text\"][0]\n",
    "wrapping_print(extract_question_prompt(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for output in pipe(raw_datasets[\"test\"].to_iterable_dataset()):\n",
    "#     print(output)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from datasets import Dataset\n",
    "\n",
    "def gen_from_iterable_dataset(iterable_ds):\n",
    "    yield from iterable_ds\n",
    "\n",
    "to_dataset = lambda iterable_ds: Dataset.from_generator(partial(gen_from_iterable_dataset, iterable_ds), features=iterable_ds.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"rotten_tomatoes\", split=\"train\", streaming=True)\n",
    "to_dataset(ds.take(2))[:2], to_dataset(ds.take(2))[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/home/mmordig/reinforcement/datasets/alpha_geo_small\"\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_dir=dataset_dir)#, nrows=10)\n",
    "next(iter(dataset[\"train\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def formatting_func(row):\n",
    "#     question = row[\"nl_statement\"]\n",
    "#     answer = row[\"fl_statement\"]\n",
    "#     return {\"text\": f\"### Question: {question} ### Answer: {answer}\"}\n",
    "# dataset = dataset.map(formatting_func)\n",
    "\n",
    "def formatting_func_batched(batch):\n",
    "    questions = batch[\"nl_statement\"]\n",
    "    answers = batch[\"fl_statement\"]\n",
    "    return {\"text\": [f\"### Question: {question} ### Answer: {answer}\" for (question, answer) in zip(questions, answers)]}\n",
    "\n",
    "dataset = dataset.map(formatting_func_batched, batched=True)\n",
    "dataset = dataset.remove_columns(set(dataset[\"train\"].column_names) - {\"text\"})\n",
    "dataset = dataset[\"train\"].train_test_split(0.1, seed=1)\n",
    "dataset_filename = \"/home/mmordig/reinforcement/HumbleAttemptAtGeneralAI/geometry_translation/new/runs/verb_dataset\"\n",
    "dataset.save_to_disk(dataset_filename)\n",
    "dataset, dataset[\"train\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset(dataset_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "dataset_dir = \"/home/mmordig/reinforcement/datasets/alpha_geo_small\"\n",
    "model_name_or_path = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side=\"left\")\n",
    "\n",
    "tokenizer(\" \")[\"input_ids\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_constant_length_dataset(dataset_dir, **kwargs):\n",
    "    # use with SFTTrainer together with arg packing=True, not compatible with DataCollatorForCompletionOnlyLM\n",
    "    \n",
    "    text_dataset = load_dataset(\"csv\", data_dir=dataset_dir, **kwargs)\n",
    "    # next(iter(text_dataset[\"train\"]))\n",
    "    \n",
    "    def formatting_func(row):\n",
    "        question = row[\"nl_statement\"]\n",
    "        answer = row[\"fl_statement\"]\n",
    "        return f\"### Question: {question} ### Answer: {answer}\"\n",
    "\n",
    "    dataset = trl.trainer.ConstantLengthDataset(tokenizer, text_dataset[\"train\"], formatting_func=formatting_func)\n",
    "    # return dataset\n",
    "    return DatasetDict({\n",
    "        \"train\": dataset,\n",
    "    })\n",
    "dataset = get_constant_length_dataset(dataset_dir)\n",
    "res = next(iter(dataset[\"train\"]))\n",
    "tokenizer.decode(res[\"input_ids\"]).split(\"### \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
