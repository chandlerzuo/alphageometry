{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mmordig/reinforcement/alphageometry/LLM_finetuner/verbalization_venv3/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys; sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pydantic import BaseModel, constr\n",
    "\n",
    "import outlines\n",
    "import torch\n",
    "\n",
    "\n",
    "class Weapon(str, Enum):\n",
    "    sword = \"sword\"\n",
    "    axe = \"axe\"\n",
    "    mace = \"mace\"\n",
    "    spear = \"spear\"\n",
    "    bow = \"bow\"\n",
    "    crossbow = \"crossbow\"\n",
    "\n",
    "\n",
    "class Armor(str, Enum):\n",
    "    leather = \"leather\"\n",
    "    chainmail = \"chainmail\"\n",
    "    plate = \"plate\"\n",
    "\n",
    "\n",
    "class Character(BaseModel):\n",
    "    name: str #constr(max_length=10)\n",
    "    age: int\n",
    "    armor: Armor\n",
    "    weapon: Weapon\n",
    "    strength: int\n",
    "\n",
    "\n",
    "# model = outlines.models.transformers(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "# model = outlines.models.transformers(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "# model = outlines.models.transformers(\"SweatyCrayfish/llama-3-8b-quantized\")\n",
    "\n",
    "# Construct structured sequence generator\n",
    "generator = outlines.generate.json(model, Character)\n",
    "\n",
    "# Draw a sample\n",
    "rng = torch.Generator(device=\"cuda\")\n",
    "rng.manual_seed(789001)\n",
    "\n",
    "character = generator(\"Give me a character description\", rng=rng)\n",
    "\n",
    "print(repr(character))\n",
    "# Character(name='Anderson', age=28, armor=<Armor.chainmail: 'chainmail'>, weapon=<Weapon.sword: 'sword'>, strength=8)\n",
    "\n",
    "character = generator(\"Give me an interesting character description\", rng=rng)\n",
    "\n",
    "print(repr(character))\n",
    "# Character(name='Vivian Thr', age=44, armor=<Armor.plate: 'plate'>, weapon=<Weapon.crossbow: 'crossbow'>, strength=125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trl\n",
    "trl.trainer.ConstantLengthDataset\n",
    "from trl.extras.dataset_formatting import conversations_formatting_function\n",
    "\n",
    "from datasets import load_dataset, load_from_disk, DatasetDict\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from typing import Annotated\n",
    "from transformers import LogitsProcessorList, TextStreamer\n",
    "import json\n",
    "\n",
    "from LLM_finetuner.constrained_generation import GrammarLogitProcessor, StatefulGrammarLogitProcessor, GrammarType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_name = \"gpt2\"\n",
    "model_name = \"SweatyCrayfish/llama-3-8b-quantized\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# load in 8-bit\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True)\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_config)\n",
    "\n",
    "# model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import outlines\n",
    "from outlines.models import Transformers, transformers\n",
    "# outlines.models.transformers()\n",
    "model_outlines = Transformers(model, tokenizer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, constr, field_validator\n",
    "from enum import Enum\n",
    "\n",
    "from typing import List, Union\n",
    "\n",
    "# class Triangle(BaseModel):\n",
    "#     vertices: List[str]\n",
    "    \n",
    "# class Quadruple(BaseModel):\n",
    "#     vertices: List[str]\n",
    "#     @field_validator(\"vertices\")\n",
    "#     def check_length(cls, v):\n",
    "#         if (len(v) != 4) or (len(set(v)) != 4):\n",
    "#             raise ValueError(\"Must be a quadruple\")\n",
    "#         return v\n",
    "\n",
    "# class GeometryCommand(BaseModel):\n",
    "#     commands: List[Union[Triangle, Quadruple]]\n",
    "#     @field_validator(\"commands\")\n",
    "#     def check_commands(cls, v):\n",
    "#         assert len(v) == 3\n",
    "\n",
    "# class Triangle(BaseModel):\n",
    "#     p1: str\n",
    "#     p2: str\n",
    "#     p3: str\n",
    "    \n",
    "# class Quadruple(BaseModel):\n",
    "#     p1: str\n",
    "#     p2: str\n",
    "#     p3: str\n",
    "#     p4: str\n",
    "\n",
    "class Weapon(str, Enum):\n",
    "    sword = \"sword\"\n",
    "    axe = \"axe\"\n",
    "    mace = \"mace\"\n",
    "    spear = \"spear\"\n",
    "    bow = \"bow\"\n",
    "    crossbow = \"crossbow\"\n",
    "\n",
    "\n",
    "class Armor(str, Enum):\n",
    "    leather = \"leather\"\n",
    "    chainmail = \"chainmail\"\n",
    "    plate = \"plate\"\n",
    "\n",
    "\n",
    "class Character(BaseModel):\n",
    "    name: constr(max_length=10)\n",
    "    age: int\n",
    "    armor: Armor\n",
    "    weapon: Weapon\n",
    "    strength: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = outlines.generate.json(model_outlines, Character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(\"Generate some geometry commands: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = tokenizer(\"one; two; one; three; \")[\"input_ids\"]\n",
    "\n",
    "tokenizer.batch_decode(xxx), xxx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsm.tokenizer(\"one; two; one; three; \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from outlines.fsm.fsm import RegexFSM, CFGGuide, CFGFSM\n",
    "\n",
    "grammar = \"\"\"\n",
    "    ?start: clauses\n",
    "    ?clauses: (clause_term)+\n",
    "    ?clause_term: clause \";\"\n",
    "    ?clause: \"one\" | \"two\" | \"three\"\n",
    "    \n",
    "    %import common.WS\n",
    "    %ignore WS\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "processor = GrammarLogitProcessor(\n",
    "    tokenizer, device=\"unspecified\", grammar=grammar, \n",
    "    # grammar_type=GrammarType.GRAMMAR_TYPE_JSON\n",
    "    grammar_type=GrammarType.GRAMMAR_TYPE_REGEX\n",
    ")\n",
    "fsm = processor.fsm\n",
    "# fsm = CFGFSM(grammar, tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "state = 0\n",
    "# i = 0\n",
    "for i in range(len(xxx)):\n",
    "    print(f\"Allowed: ...{fsm.allowed_token_ids(state)[-10:]}, choosing {xxx[i]}, valid: {xxx[i] in fsm.allowed_token_ids(state)}\")\n",
    "    state = fsm.next_state(state, xxx[i])\n",
    "    print(f\"Moved to state {state}\")\n",
    "# state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lark import Lark\n",
    "json_parser = Lark(r\"\"\"\n",
    "    ?value: dict\n",
    "         | list\n",
    "         | string\n",
    "         | SIGNED_NUMBER -> number\n",
    "         | \"true\" -> true | \"false\" -> false | \"null\" -> null\n",
    "\n",
    "    string: ESCAPED_STRING\n",
    "    list : \"[\" [value (\",\" value)*] \"]\"\n",
    "\n",
    "    dict : \"{\" [pair (\",\" pair)*] \"}\"\n",
    "    pair : string \":\" value\n",
    "\n",
    "    %import common.ESCAPED_STRING\n",
    "    %import common.SIGNED_NUMBER\n",
    "    %import common.WS\n",
    "    %ignore WS\n",
    "\n",
    "    \"\"\", start='value', parser=\"lalr\")\n",
    "\n",
    "from lark import Transformer\n",
    "\n",
    "from lark import Transformer\n",
    "\n",
    "class TreeToJson(Transformer):\n",
    "    def string(self, s):\n",
    "        (s,) = s\n",
    "        return s[1:-1]\n",
    "    def number(self, n):\n",
    "        (n,) = n\n",
    "        return float(n)\n",
    "\n",
    "    list = list\n",
    "    pair = tuple\n",
    "    dict = dict\n",
    "\n",
    "    null = lambda self, _: None\n",
    "    true = lambda self, _: True\n",
    "    false = lambda self, _: False\n",
    "    \n",
    "text = '{\"key\": [\"item0\", \"item1\", 3.14, null, false]}'\n",
    "tree = json_parser.parse(text)\n",
    "print( tree.pretty() )\n",
    "TreeToJson().transform(tree)\n",
    "# _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lark import Lark\n",
    "from outlines import grammars\n",
    "def LarkV1(cfg_string):\n",
    "    return Lark(\n",
    "            cfg_string,\n",
    "            parser=\"lalr\",\n",
    "            lexer=\"contextual\",\n",
    "            propagate_positions=False,\n",
    "            maybe_placeholders=False,\n",
    "            regex=True,\n",
    "            import_paths=[grammars.GRAMMAR_PATH],\n",
    "        )\n",
    "json_parser = LarkV1(r\"\"\"\n",
    "    ?start: clauses\n",
    "    ?clauses: (clause_term)+ -> clauses\n",
    "    ?clause_term: clause \";\" -> clause_term\n",
    "    ?clause: \"one\" -> one | \"two\" -> two | \"three\" -> three\n",
    "    \n",
    "    %import common.WS\n",
    "    %ignore WS\n",
    "\"\"\")\n",
    "x1 = json_parser.parse((\"one; two; one; three; \"))\n",
    "print(x1.pretty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"\"\"\n",
    "    ?start: clauses\n",
    "    ?clauses: (clause_term)+\n",
    "    ?clause_term: clause \";\"\n",
    "    ?clause: \"one\" | \"two\" | \"three\"\n",
    "    \n",
    "    %import common.WS\n",
    "    %ignore WS\n",
    "\"\"\"\n",
    "\n",
    "processor = GrammarLogitProcessor(\n",
    "    tokenizer, device=\"unspecified\", grammar=grammar, \n",
    "    # grammar_type=GrammarType.GRAMMAR_TYPE_JSON\n",
    "    grammar_type=GrammarType.GRAMMAR_TYPE_REGEX\n",
    ")\n",
    "xxx = processor.tokenizer(\"one; two; one; three; \")[\"input_ids\"]\n",
    "# xxx = processor.tokenizer(\"start : \")[\"input_ids\"]\n",
    "\n",
    "fsm = processor.fsm\n",
    "xxx, fsm.allowed_token_ids(state=0)\n",
    "\n",
    "# fsm.allowed_token_ids(5)\n",
    "\n",
    "processor.tokenizer.batch_decode(xxx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "    ?start: value\n",
    "    ?value: dict\n",
    "         | list\n",
    "         | string\n",
    "         | SIGNED_NUMBER -> number\n",
    "         | \"true\" -> true | \"false\" -> false | \"null\" -> null\n",
    "\n",
    "    string: UNESCAPED_STRING\n",
    "    list : \"[\" [value (\",\" value)*] \"]\"\n",
    "\n",
    "    dict : \"{\" [pair (\",\" pair)*] \"}\"\n",
    "    pair : string \":\" value\n",
    "\n",
    "    %import common.UNESCAPED_STRING\n",
    "    %import common.SIGNED_NUMBER\n",
    "    %import common.WS\n",
    "    %ignore WS\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "# processor = GrammarLogitProcessor(\n",
    "#     tokenizer, device=\"unspecified\", grammar=grammar, \n",
    "#     # grammar_type=GrammarType.GRAMMAR_TYPE_JSON\n",
    "#     grammar_type=GrammarType.GRAMMAR_TYPE_REGEX\n",
    "# )\n",
    "\n",
    "xxx = processor.tokenizer('{\"key\": [\"item0\", \"item1\", 3.14, null, false]}')[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state = 0\n",
    "for token_id in xxx:\n",
    "    next_word, *allowed_words = processor.tokenizer.convert_ids_to_tokens([token_id] + processor.fsm.allowed_token_ids(state))\n",
    "    print(f\"Allowed: {allowed_words}\")\n",
    "    print(f\"Chose {next_word}\")\n",
    "    state = processor.advance(token_id, state)\n",
    "    print(f\"State: {state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from outlines import models\n",
    "# from outlines import generate\n",
    "\n",
    "# def add(a: int, b: int):\n",
    "#     return a + b\n",
    "\n",
    "# model = models.transformers(\"mistralai/Mistral-7B-v0.1\")\n",
    "# generator = generate.json(model, add)\n",
    "# result = generator(\"Return two integers named a and b respectively. a is odd and b even.\")\n",
    "\n",
    "# print(add(**result))\n",
    "# # 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pydantic import BaseModel, conint, Field\n",
    "class CarSpecs(BaseModel):\n",
    "    seller: str\n",
    "    model: str\n",
    "    year: int\n",
    "    price: Annotated[int, Field(strict=True, ge=0)] #conint(ge=0)\n",
    "    # year: conint(ge=1900, le=2022)\n",
    "    # year: int\n",
    "\n",
    "# grammar = CarSpecs.model_json_schema()\n",
    "# grammar = json.dumps(grammar)\n",
    "\n",
    "# text = f\"Generate JSON output for this schema: {grammar}\"\n",
    "# text = [\n",
    "#     {\n",
    "#         \"role\": \"system\",\n",
    "#         \"content\": text\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# tokenizer.padding_side = \"left\"\n",
    "# text = tokenizer.apply_chat_template(text, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "# grammar = \"\"\"\n",
    "#     ?start: expression\n",
    "\n",
    "#     ?expression: term ((\"+\" | \"-\") term)*\n",
    "\n",
    "#     ?term: factor ((\"*\" | \"/\") factor)*\n",
    "\n",
    "#     ?factor: NUMBER\n",
    "#            | \"-\" factor\n",
    "#            | \"(\" expression \")\"\n",
    "\n",
    "#     %import common.NUMBER\n",
    "# \"\"\"\n",
    "# text = (\n",
    "#     \"Alice had 4 apples and Bob ate 2. \"\n",
    "#   + \"Write an expression for Alice's apples:\"\n",
    "# )\n",
    "grammar = \"\"\"\n",
    "?start: clauses\n",
    "?clauses: clause_term+\n",
    "?clause_term: clause \";\"\n",
    "?clause: \"one\" | \"two\" | \"three\"\n",
    "\"\"\"\n",
    "# grammar = \"\"\"\n",
    "#     ?start: expression\n",
    "\n",
    "#     ?expression: term ((\"+\" | \"-\") term)*\n",
    "\n",
    "#     ?term: factor ((\"*\" | \"/\") factor)*\n",
    "\n",
    "#     ?factor: NUMBER\n",
    "#            | \"-\" factor\n",
    "#            | \"(\" expression \")\"\n",
    "\n",
    "#     %import common.NUMBER\n",
    "# \"\"\"\n",
    "text = \"Write some text using one, two, three:\"\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "streamer = TextStreamer(tokenizer)\n",
    "grammar_logits_processor = StatefulGrammarLogitProcessor(GrammarLogitProcessor(\n",
    "    tokenizer, device=\"unspecified\", grammar=grammar, \n",
    "    # grammar_type=GrammarType.GRAMMAR_TYPE_JSON\n",
    "    grammar_type=GrammarType.GRAMMAR_TYPE_REGEX\n",
    "))\n",
    "encoded_inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")#, padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor.tokenizer.convert_ids_to_tokens(xxx)\n",
    "processor.fsm.allowed_token_ids(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_logits_processor.fsm_states = None\n",
    "grammar_logits_processor.cur_seq_len = None\n",
    "print(\"Starting generation:\")\n",
    "_ = model.generate(**encoded_inputs, logits_processor=LogitsProcessorList([grammar_logits_processor]), num_beams=2 if streamer is None else 1, max_new_tokens=40, streamer=streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from outlines import models, generate\n",
    "from outlines.models import Transformers\n",
    "\n",
    "arithmetic_grammar = \"\"\"\n",
    "    ?start: expression\n",
    "\n",
    "    ?expression: term ((\"+\" | \"-\") term)*\n",
    "\n",
    "    ?term: factor ((\"*\" | \"/\") factor)*\n",
    "\n",
    "    ?factor: NUMBER\n",
    "           | \"-\" factor\n",
    "           | \"(\" expression \")\"\n",
    "\n",
    "    %import common.NUMBER\n",
    "\"\"\"\n",
    "\n",
    "# model = models.transformers(\"WizardLM/WizardMath-7B-V1.1\")\n",
    "# model.tokenizer = tokenizer\n",
    "generator = generate.cfg(Transformers(model, tokenizer), arithmetic_grammar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = generator(\n",
    "  \"Alice had 4 apples and Bob ate 2. \"\n",
    "  + \"Write an expression for Alice's apples:\"\n",
    ")\n",
    "\n",
    "# pip install git+https://github.com/outlines-dev/outlines.git pycountry pyairports\n",
    "print(sequence)\n",
    "# (8-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = generator(\n",
    "  \"Alice had 4 apples and Bob ate 2. \"\n",
    "  + \"Write an expression for Alice's apples:\"\n",
    ")\n",
    "\n",
    "print(sequence)\n",
    "# (8-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/mmordig/reinforcement/alphageometry/defs.txt\", \"r\") as f:\n",
    "    for (i, line) in enumerate(f):\n",
    "        if i % 6 != 0: continue\n",
    "        line = line.strip()\n",
    "        # print(line)\n",
    "        fun, *args= line.split()\n",
    "        print(fun, len(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "# print(tokenizer.pad_token)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "import functools\n",
    "from datasets.fingerprint import Hasher\n",
    "Hasher.hash(tokenizer)\n",
    "tokenizer\n",
    "\n",
    "def ggg(a, b):\n",
    "    return a + b\n",
    "functools.partial(ggg, b=2)(1)\n",
    "Hasher.hash(functools.partial(ggg, b=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "GenerationConfig(aaax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "Hasher.hash(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "\n",
    "\n",
    "extra_tokens_file = Path(os.path.expanduser(\"~/reinforcement/alphageometry/assets/def-patterns-desc.yml\"))\n",
    "def_to_desc = yaml.safe_load(extra_tokens_file.read_text())\n",
    "tokenizer.add_tokens([defn for defn in def_to_desc.keys()], special_tokens=True)\n",
    "# tokenizer.tokenize(\"<s>[INST] ABCD is a trapezoid. E is a points such that line CB meets line AD at E. Define points G, H, F, I such that line FG and line HI are common tangents to circle centered at B with radius BD and circle centered at E with radius EC at points F, G and H, I respectively.. J is defined such that ∠JCE is equal to ∠GIF. Let K be a point such that ∠KED is equal to ∠GHC. [/INST] A B C D = [trapezoid] A B C D; E = [intersection_ll] E C B A D; F G H I = [cc_tangent] F G H I B D E C; J = [eqangle3] J C E G I F; K = [eqangle3] K E D G H C </s>[END]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from LLM_finetuner.dataset import convert_formalalphageom_to_json\n",
    "\n",
    "\n",
    "dataset_dir = \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/runs/verbalization/datasets/alpha_geo_small_arrow\"\n",
    "dataset = load_dataset(dataset_dir)\n",
    "\n",
    "def json_converter(batch):\n",
    "    return {\"fl_statement\": [convert_formalalphageom_to_json(x) for x in batch[\"fl_statement\"]]}\n",
    "# logger.info(\"Converting to JSON\")\n",
    "dataset[\"train\"].map(json_converter, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_formalalphageom_to_json(expression, pretty=False):\n",
    "    parts = expression.split(\"; \")\n",
    "    result = []\n",
    "\n",
    "    for part in parts:\n",
    "        defines, func_call = part.split(\" = \")\n",
    "        func_name, *args = func_call.split()\n",
    "        defines_list = defines.split()\n",
    "\n",
    "        func_dict = {\n",
    "            \"fun\": func_name,\n",
    "            \"args\": args,\n",
    "            \"defines\": defines_list\n",
    "        }\n",
    "\n",
    "        result.append(func_dict)\n",
    "\n",
    "    # return json.dumps(result)\n",
    "    return json.dumps(result, indent=2) if pretty else result\n",
    "\n",
    "# Example usage\n",
    "expression = 'A B C = ieq_triangle A B C; D E F = risos D E F; G = psquare G A E; H = intersection_cc H C G E; I J = tangent I J C B F; K = intersection_tt K J D I E A G'\n",
    "def show_transformation(expression):\n",
    "    print(expression)\n",
    "    print(\"Translation:\")\n",
    "    json_output = convert_formalalphageom_to_json(expression, pretty=True)\n",
    "    # json_output = convert_formalalphageom_to_json(expression, pretty=False)\n",
    "    print(json_output)\n",
    "show_transformation(expression)\n",
    "# print(convert_to_json(dataset[\"train\"][1][\"fl_statement\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "?accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "# accelerator = Accelerator()\n",
    "# accelerator.accumulate(model)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "def gen():\n",
    "    yield {\"pokemon\": \"bulbasaur\", \"type\": \"grass\"}\n",
    "    yield {\"pokemon\": \"squirtle\", \"type\": \"water\"}\n",
    "ds = Dataset.from_generator(gen)\n",
    "ds[0]\n",
    "\n",
    "from datasets import IterableDataset\n",
    "ds = IterableDataset.from_generator(gen)\n",
    "for example in ds:\n",
    "    print(example)\n",
    "for example in ds:\n",
    "    print(example)\n",
    "    \n",
    "# ds = Dataset.from_dict()\n",
    "ds.set_epoch\n",
    "# IterableDataset.from_generator((x for x in range(10))) # not a generator function, but a generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "ds = Dataset.from_dict({\"aa\": [0, 1], \"bb\": [2, 3]})\n",
    "ds.map(lambda x: {\"cc\": x[\"aa\"] + 10}, load_from_cache_file=False, keep_in_memory=False)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset\n",
    "dataset[\"train\"][0]\n",
    "dataset[\"train\"][:2]\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LLM_finetuner.dataset import replace_words_with_tokens, format_question_answer_single\n",
    "\n",
    "\n",
    "extra_tokens_file = Path(os.path.expanduser(\"~/reinforcement/alphageometry/assets/def-patterns-desc.yml\"))\n",
    "def_to_desc = yaml.safe_load(extra_tokens_file.read_text())\n",
    "# eq_triangle, sieq_triangle, so replace by longest first\n",
    "geom_tokens = sorted(list(def_to_desc.keys()), key=lambda x: -len(x))\n",
    "print(geom_tokens)\n",
    "\n",
    "dataset = dataset.map(lambda x: {\"fl_statement\": replace_words_with_tokens(x[\"fl_statement\"], geom_tokens)}, num_proc=8)\n",
    "dataset[\"train\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DataCollatorWithPadding\n",
    "?tokenizer.pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see transformers.Trainer.evaluation_loop\n",
    "# from transformers import Trainer, TrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "import sys\n",
    "from LLM_finetuner.sft_finetuning import main as get_trainer\n",
    "import LLM_finetuner\n",
    "LLM_finetuner.sft_finetuning.TRL_USE_RICH = False\n",
    "\n",
    "sys.argv = [\"--overwrite_output_dir\", \"--use_peft\", \"--per_device_train_batch_size\", \"64\", \"--per_device_eval_batch_size\", \"64\", \"--model_name_or_path\", \"gpt2\", \"--eval_steps\", \"10\", \"--evaluation_strategy\", \"steps\", \"--max_eval_samples\", \"2\", \"--explicit_eos_str\", \"[END]\", \"--extra_tokens_file\", \"/home/mmordig/reinforcement/alphageometry/assets/def-patterns-desc.yml\", \"--output_dir\", \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/runs/verbalization/training/debug112/{model_name}_{max_train_samples}ex_peft{use_peft}\", \"--dataset_name\", \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/runs/verbalization/datasets/alpha_geo_processed\", \"--config\", \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/trl_sft_config.yml\", \"--max_train_samples\", \"10\", \"--num_train_epochs\", \"100000\",]\n",
    "sys.argv += [\"--include_inputs_for_metrics\"]\n",
    "# sys.argv += [\"--dataloader_num_workers: 0\"]\n",
    "trainer, resume_checkpoint = get_trainer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "def compute_metrics(eval_results):\n",
    "    breakpoint()\n",
    "    predictions = eval_results.predictions\n",
    "    inputs = eval_results.inputs\n",
    "    labels = eval_results.label_ids\n",
    "    # print(predictions)\n",
    "    print(inputs[0])\n",
    "    print(labels[0])\n",
    "    print(predictions[0])\n",
    "    predictions = trainer.tokenizer.batch_decode(predictions)\n",
    "    inputs = trainer.tokenizer.batch_decode(inputs)\n",
    "    labels = trainer.tokenizer.batch_decode(labels)\n",
    "    print(predictions)\n",
    "    print(labels)\n",
    "    # print(predictions.shape)\n",
    "    # return {\"generated_text\": predictions}\n",
    "    # return {\"generated_text2\": wandb.Table(columns=[\"text\"], data=[[p] for p in predictions])}\n",
    "    return {\"predictions\": wandb.Table(columns=[\"input\", \"predicted\"], data=[[i, p] for (i, p) in zip(inputs, predictions)])}\n",
    "    \n",
    "trainer.compute_metrics = compute_metrics\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TrainerWithGeneration(Trainer):\n",
    "#     def generate_with_model(self):\n",
    "#         model = self._wrap_model(self.model, training=False, dataloader=dataloader)\n",
    "#         model.eval()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class MakePredictionsCallback(TrainerCallback):\n",
    "    def __init__(self, max_generation_length=1024):\n",
    "        # self.tokenizer = tokenizer\n",
    "        self.max_generation_length = max_generation_length\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, model, eval_dataloader, **kwargs):\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        for batch in eval_dataloader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            with torch.no_grad():\n",
    "                output = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=self.max_generation_length)\n",
    "            predictions.extend(output)\n",
    "        # model.train()\n",
    "        \n",
    "        control.on_lo\n",
    "        return control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from contextlib import nullcontext\n",
    "\n",
    "# from torchdistx.fake import fake_mode\n",
    "# with fake_mode():\n",
    "with nullcontext():\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1, 20, 5),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(20, 64, 5),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "    total_activations = 0\n",
    "\n",
    "    def count_activations_hook(module, input, output):\n",
    "        global total_activations\n",
    "        activations = output.numel()\n",
    "        total_activations += activations\n",
    "        print(f\"{module.__class__.__name__} produced {activations} activations\")\n",
    "\n",
    "    for layer in model.children():\n",
    "        layer.register_forward_hook(count_activations_hook)\n",
    "\n",
    "    input_tensor = torch.randn(1, 1, 28, 28)\n",
    "    model(input_tensor)\n",
    "    print(f\"Total activations: {total_activations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "from accelerate import init_empty_weights\n",
    "model_name = \"gpt2\"\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model(input_ids=torch.LongTensor([1, 2, 3])[None, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(model)\n",
    "# model.modules[0]\n",
    "next(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_activations = 0\n",
    "\n",
    "def count_activations_hook(module, input, output):\n",
    "    global total_activations\n",
    "    # output.shape[0] is the batch size\n",
    "    # output.numel() gives the total number of elements in the output, for all batches\n",
    "    # To get activations per single input, you would divide by output.shape[0]\n",
    "    activations = output.numel()\n",
    "    total_activations += activations\n",
    "    print(f\"{module.__class__.__name__} produced {activations} activations\")\n",
    "\n",
    "# Register hook for each layer\n",
    "for layer in model.children():\n",
    "    layer.register_forward_hook(count_activations_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "print(accelerator.state)\n",
    "print(str(accelerator))\n",
    "print(f\"{accelerator=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "    \n",
    "# r'''one \\\"two\\\" 'three''''\n",
    "# parse_htcondor_arguments('''\"3 simple arguments\"''')\n",
    "\n",
    "from accelerate import init_empty_weights\n",
    "\n",
    "\n",
    "# format_htcondor_arguments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate\n",
    "accelerate.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple(\"$(Cluster) $(Process) $(NumRanks)\".split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LLM_finetuner.utils import add_new_tokens_with_average_init\n",
    "\n",
    "\n",
    "extra_tokens_file = Path(os.path.expanduser(\"~/reinforcement/alphageometry/assets/def-patterns-desc.yml\"))\n",
    "def_to_desc = yaml.safe_load(extra_tokens_file.read_text())\n",
    "\n",
    "add_new_tokens_with_average_init(model, tokenizer, def_to_desc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def_to_desc\n",
    "tokens = list(def_to_desc.keys())\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"The following tokens are already known, ignoring them: {[token for (token, id) in zip(tokens, token_ids) if id != tokenizer.unk_token_id]}\")\n",
    "def_to_desc = {token: def_to_desc[token] for (token, id) in zip(tokens, token_ids) if id == tokenizer.unk_token_id}\n",
    "def_to_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/home/mmordig/reinforcement/HumbleAttemptAtGeneralAI/runs/verbalization/datasets/alpha_geo_small_processed\"\n",
    "raw_datasets = load_from_disk(dataset_dir)\n",
    "dataset = raw_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/76446228/setting-padding-token-as-eos-token-when-using-datacollatorforlanguagemodeling-fr\n",
    "\n",
    "\n",
    "model_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model_name_or_path = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)#, add_eos_token=True)\n",
    "# inputs = \"Hello world\"\n",
    "inputs = [\n",
    "    {\"role\": \"system\", \"content\": \"System\"},\n",
    "    # must be ordered as user-assistant alternating sequence\n",
    "    {\"role\": \"user\", \"content\": \"Question1\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Answer1\"},\n",
    "    {\"role\": \"user\", \"content\": \"Question2\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Answer2\"},\n",
    "]\n",
    "print(tokenizer.apply_chat_template(inputs, tokenize=False, add_generation_prompt=True))\n",
    "print(\"#\"*80)\n",
    "\n",
    "inputs = [\n",
    "    {\"role\": \"system\", \"content\": \"System\"},\n",
    "    # must be ordered as user-assistant alternating sequence\n",
    "    {\"role\": \"user\", \"content\": \"Question1\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Answer1\"},\n",
    "    {\"role\": \"user\", \"content\": \"Question2\"},\n",
    "    # {\"role\": \"assistant\", \"content\": \"Answer2\"},\n",
    "]\n",
    "print(tokenizer.apply_chat_template(inputs, tokenize=False, add_generation_prompt=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"### Question: Points B, A, C are defined such that triangle ABC is an equilateral triangle. Define points D, F, & E such that E, D, and F is a right angle isosceles triangle with the right angle at D. Point G is defined such that G, A, E are three consecutive vertices of a square. Circle centered at C with radius CE intersects circle centered at G with radius GE at H and E. Points I and J are defined such that line IC and line JC are the two tangents to circle centered at B with radius BF at point I and J respectively.. Define point K such that line DI and line AG are parallel. line JK perpendicular to line DI. line EK perpendicular to line AG. line JK meets line EK at the point K. ### Answer: A B C = ieq_triangle A B C; D E F = risos D E F; G = psquare G A E; H = intersection_cc H C G E; I J = tangent I J C B F; K = intersection_tt K J D I E A G;\"\n",
    "tokenizer.decode(tokenizer(query)[\"input_ids\"], skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(query)\n",
    "\" \".join(tokenizer.tokenize(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "\n",
    "def print_to_str(x):\n",
    "    temp_out = io.StringIO()\n",
    "    print(x, file=temp_out)\n",
    "    return temp_out.getvalue()\n",
    "print(print_to_str(\"ss\\na\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LLM_finetuner.question_answer_utils import get_question_answer_to_chat_formatter\n",
    "\n",
    "formatting_func = get_question_answer_to_chat_formatter(tokenizer, text_column=\"text\")\n",
    "print(f\"Example format: {formatting_func(dataset[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geometry_translation.new.question_answer_utils import extract_question_answer\n",
    "\n",
    "\n",
    "\n",
    "text_column = \"text\"\n",
    "print(convert_question_answer_to_chat_format(dataset[0], text_column=text_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs = tokenizer(, return_tensors=\"pt\", add_special_tokens=True)\n",
    "encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(encoded_inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from datasets import load_dataset\n",
    "\n",
    "def wrapping_print(x):\n",
    "    print(textwrap.fill(x))\n",
    "    \n",
    "\n",
    "\n",
    "# dataset_dir = \"/fast/mmordig/general_ai_rl/alphageom_project/alpha_geo_processed\"\n",
    "dataset_dir = \"/home/mmordig/reinforcement/HumbleAttemptAtGeneralAI/geometry_translation/new/runs/verb_dataset\"\n",
    "\n",
    "raw_datasets = load_from_disk(dataset_dir)\n",
    "dataset = raw_datasets[\"test\"][:100]\n",
    "for row in dataset[\"text\"]:\n",
    "    # wrapping_print(row)\n",
    "    wrapping_print(extract_question_prompt(row) + \"END\")\n",
    "    print(\"Answer\")\n",
    "    wrapping_print(extract_answer(row) + \"END\")\n",
    "    break\n",
    "# question = format_question_answer(question, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from peft import AutoPeftModelForCausalLM\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "# from gradio.pipelines import load_from_pipeline\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "# output_dir = sys.argv[1]\n",
    "output_dir = \"runs/trl_sft\"\n",
    "model_name_or_path = get_last_checkpoint(output_dir)\n",
    "logger.info(f\"Loading model from '{model_name_or_path}'\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\", load_in_4bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "logger.info(f\"Loaded model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, num_return_sequences=2, num_beams=4, do_sample=True, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ?pipe\n",
    "\n",
    "row = dataset[\"text\"][0]\n",
    "wrapping_print(extract_question_prompt(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for output in pipe(raw_datasets[\"test\"].to_iterable_dataset()):\n",
    "#     print(output)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from datasets import Dataset\n",
    "\n",
    "def gen_from_iterable_dataset(iterable_ds):\n",
    "    yield from iterable_ds\n",
    "\n",
    "to_dataset = lambda iterable_ds: Dataset.from_generator(partial(gen_from_iterable_dataset, iterable_ds), features=iterable_ds.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"rotten_tomatoes\", split=\"train\", streaming=True)\n",
    "to_dataset(ds.take(2))[:2], to_dataset(ds.take(2))[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/home/mmordig/reinforcement/datasets/alpha_geo_small\"\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_dir=dataset_dir)#, nrows=10)\n",
    "next(iter(dataset[\"train\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def formatting_func(row):\n",
    "#     question = row[\"nl_statement\"]\n",
    "#     answer = row[\"fl_statement\"]\n",
    "#     return {\"text\": f\"### Question: {question} ### Answer: {answer}\"}\n",
    "# dataset = dataset.map(formatting_func)\n",
    "\n",
    "def formatting_func_batched(batch):\n",
    "    questions = batch[\"nl_statement\"]\n",
    "    answers = batch[\"fl_statement\"]\n",
    "    return {\"text\": [f\"### Question: {question} ### Answer: {answer}\" for (question, answer) in zip(questions, answers)]}\n",
    "\n",
    "dataset = dataset.map(formatting_func_batched, batched=True)\n",
    "dataset = dataset.remove_columns(set(dataset[\"train\"].column_names) - {\"text\"})\n",
    "dataset = dataset[\"train\"].train_test_split(0.1, seed=1)\n",
    "dataset_filename = \"/home/mmordig/reinforcement/HumbleAttemptAtGeneralAI/geometry_translation/new/runs/verb_dataset\"\n",
    "dataset.save_to_disk(dataset_filename)\n",
    "dataset, dataset[\"train\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset(dataset_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "dataset_dir = \"/home/mmordig/reinforcement/datasets/alpha_geo_small\"\n",
    "model_name_or_path = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side=\"left\")\n",
    "\n",
    "tokenizer(\" \")[\"input_ids\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_constant_length_dataset(dataset_dir, **kwargs):\n",
    "    # use with SFTTrainer together with arg packing=True, not compatible with DataCollatorForCompletionOnlyLM\n",
    "    \n",
    "    text_dataset = load_dataset(\"csv\", data_dir=dataset_dir, **kwargs)\n",
    "    # next(iter(text_dataset[\"train\"]))\n",
    "    \n",
    "    def formatting_func(row):\n",
    "        question = row[\"nl_statement\"]\n",
    "        answer = row[\"fl_statement\"]\n",
    "        return f\"### Question: {question} ### Answer: {answer}\"\n",
    "\n",
    "    dataset = trl.trainer.ConstantLengthDataset(tokenizer, text_dataset[\"train\"], formatting_func=formatting_func)\n",
    "    # return dataset\n",
    "    return DatasetDict({\n",
    "        \"train\": dataset,\n",
    "    })\n",
    "dataset = get_constant_length_dataset(dataset_dir)\n",
    "res = next(iter(dataset[\"train\"]))\n",
    "tokenizer.decode(res[\"input_ids\"]).split(\"### \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
