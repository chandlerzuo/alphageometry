{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmordig/reinforcement/alphageometry/LLM_finetuner/verbalization_venv3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict\n",
    "import torch\n",
    "import more_itertools\n",
    "import logging\n",
    "import sys\n",
    "import textwrap\n",
    "import collections\n",
    "\n",
    "\n",
    "import tqdm\n",
    "from transformers import pipeline, HfArgumentParser\n",
    "from datasets import load_from_disk\n",
    "import gradio as gr\n",
    "from contextlib import nullcontext, redirect_stdout\n",
    "\n",
    "from LLM_finetuner.question_answer_utils import extract_answer, extract_question_prompt, get_question_answer_to_chat_formatter\n",
    "from LLM_finetuner.utils import load_model_for_inference, setup_logging, subset_dataset\n",
    "\n",
    "#%%\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "setup_logging()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import LLM_finetuner\n",
    "import LLM_finetuner.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.warning(\"Using dummy args\")\n",
    "# model_name_or_path = \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/runs/verbalization/training/overfit_single_nocompl/gpt2\"\n",
    "# model_name_or_path = \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/runs/verbalization/training/overfit_single_nocompl/gpt2_2ex\"\n",
    "# model_name_or_path = \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/runs/verbalization/training/overfit_single_nocompl/gpt2_withpeft\"\n",
    "model_name_or_path = \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/cluster_runs/verbalization/training/failed_quote/gpt2_1000ex_peftFalse\"\n",
    "# model_name_or_path = \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/cluster_runs/verbalization/training/failed_quote/gpt2_1000ex_peftTrue\" # broken\n",
    "model_name_or_path = \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/cluster_runs/verbalization/training/failed_quote/Llama-2-7b-chat-hf_1000ex_peftTrue\"\n",
    "dataset_name = \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/runs/verbalization/datasets/alpha_geo_small_processed\"\n",
    "# dataset_test_name = \"test\"\n",
    "dataset_test_name = \"train\" # for overfitting exp\n",
    "filename_predictions_out = \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/runs/verbalization/predictions/exp_small/gpt2_predictions.txt\"\n",
    "# max_predict_samples = 2\n",
    "max_predict_samples = 2\n",
    "dataset_text_field = \"text\"\n",
    "max_new_tokens = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Generating predictions, writing to file '{filename_predictions_out}'\")\n",
    "\n",
    "raw_datasets = load_from_disk(dataset_name)\n",
    "dataset = raw_datasets[dataset_test_name]\n",
    "dataset = subset_dataset(dataset, n_samples=max_predict_samples)\n",
    "\n",
    "model, tokenizer = load_model_for_inference(model_name_or_path)\n",
    "\n",
    "is_chat_model = tokenizer.chat_template is not None\n",
    "if is_chat_model:\n",
    "    logger.info(\"Detected chat model, formatting according to chat template\")\n",
    "    # assumes user-assistant roles\n",
    "    prompt_extraction_function = get_question_answer_to_chat_formatter(tokenizer, text_column=None, add_generation_prompt=True)\n",
    "else:\n",
    "    prompt_extraction_function = extract_question_prompt\n",
    "    \n",
    "def extract_extra_cols(batch):\n",
    "    return {\n",
    "        \"question_prompt\": [prompt_extraction_function(item) for item in batch[dataset_text_field]],\n",
    "        \"answer_only\": [extract_answer(item) for item in batch[dataset_text_field]],\n",
    "    }\n",
    "dataset = dataset.map(extract_extra_cols, batched=True)\n",
    "logger.info(f\"Example datapoint: {dataset[0]}\")\n",
    "\n",
    "# use_cache to avoid recomputing hidden states, see https://discuss.huggingface.co/t/what-is-the-purpose-of-use-cache-in-decoder/958\n",
    "# max_new_tokens = 70\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, \n",
    "    num_return_sequences=2, num_beams=4, do_sample=True, max_new_tokens=max_new_tokens, use_cache=True,\n",
    "    return_full_text=False, # answer only\n",
    "    num_workers=2, \n",
    "    # batch_size=2 # triggers a cuda device-side error, maybe related to https://github.com/huggingface/transformers/issues/22546\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# model_name_or_path = \"gpt2\"\n",
    "# model_name_or_path = \"meta-llama/Llama-2-7b-hf\"\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False, add_eos_token=True, pad_token=\"[PAD]\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True, add_eos_token=True)\n",
    "# tokenizer.pad_token = \"[PAD]\"\n",
    "# print(tokenizer.eos_token)\n",
    "# print(tokenizer.pad_token)\n",
    "\n",
    "# # ?tokenizer\n",
    "# tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_prompt = dataset[0][\"question_prompt\"]\n",
    "# text = dataset[0][\"text\"]\n",
    "# tokenizer.decode(tokenizer(text)[\"input_ids\"], include_special_tokens=True)[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break_nicely = lambda x: \"\\u23CE\\n\".join(textwrap.wrap(x)) # symbol \"‚èé\" for line breaks\n",
    "# using a pipe/dataset is faster because GPU works in the background while writing to file\n",
    "# with open(filename_predictions_out, 'w') as f, redirect_stdout(f):\n",
    "# with nullcontext():\n",
    "for (out, question_prompt, gt_answer) in tqdm.tqdm(zip(pipe(dataset[\"question_prompt\"]), dataset[\"question_prompt\"], dataset[\"answer_only\"])):\n",
    "    print(\"#\"*80)\n",
    "    print(\"Query: \")\n",
    "    print(break_nicely(question_prompt))\n",
    "    print(\"Expected answer: \")\n",
    "    print(break_nicely(gt_answer))\n",
    "    # strips whitespace because generated text has leading and trailing whitespace\n",
    "    out_counted = collections.Counter([candidate[\"generated_text\"].strip() for candidate in out])\n",
    "    gt_answer = gt_answer.strip()\n",
    "    print(f\"Number of candidates that are equal to expected: {out_counted.get(gt_answer, 0)}\")\n",
    "    print(f\"Number of candidates that begin with expected:\", sum(out_counted[key] for key in out_counted if key.startswith(gt_answer)))\n",
    "    # for (i, candidate) in enumerate(out):\n",
    "    #     candidate_text = candidate[\"generated_text\"]\n",
    "    for (i, (candidate_text, count)) in enumerate(out_counted.items()):\n",
    "        # logger.info(f\"Generated text: {candidate_text}\")\n",
    "        # answer = extract_answer(candidate_text)\n",
    "        answer = candidate_text\n",
    "        print(\"#\"*20 + f\" Candidate {i+1} (appears {count} times) \" + \"#\"*20)\n",
    "        extra = \"\"\n",
    "        # not perfect because tokenizing with question_prompt may lead to different tokenization\n",
    "        if len(tokenizer(answer)[\"input_ids\"]) == max_new_tokens:\n",
    "            extra = \" <MAX token length exceeded>\"\n",
    "        print(break_nicely(answer) + extra)\n",
    "    # sys.stdout.flush()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(out_counted.keys())[0])\n",
    "print(' ' + gt_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(out_counted.keys())[0] == (' ' + gt_answer)\n",
    "def show_diff(s1, s2):\n",
    "    for (c1, c2) in zip(s1, s2):\n",
    "        if c1 != c2:\n",
    "            print(c1, c2)\n",
    "show_diff(list(out_counted.keys())[0], (' ' + gt_answer))\n",
    "\n",
    "s1 = list(out_counted.keys())[0]\n",
    "s2 = (' ' + gt_answer)\n",
    "s1[len(s2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[0][\"text\"]\n",
    "question_prompt = \"### Question: Points B, A, C are defined such that triangle ABC is an equilateral triangle. Define points D, F, & E such that E, D, and F is a right angle isosceles triangle with the right angle at D. Point G is defined such that G, A, E are three consecutive vertices of a square. Circle centered at C with radius CE intersects circle centered at G with radius GE at H and E. Points I and J are defined such that line IC and line JC are the two tangents to circle centered at B with radius BF at point I and J respectively.. Define point K such that line DI and line AG are parallel. line JK perpendicular to line DI. line EK perpendicular to line AG. line JK meets line EK at the point K. ### Answer:\"\n",
    "print(tokenizer(question_prompt + \" \")[\"input_ids\"][-10:] + tokenizer(\"answer\")[\"input_ids\"])\n",
    "print(tokenizer(question_prompt + \" answer\")[\"input_ids\"][-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset_text_field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer(\"Hello answer\")[\"input_ids\"])\n",
    "print(tokenizer(\"Hello answer \")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.generate(**tokenizer(\"Hello answer\", return_tensors=\"pt\").to(\"cuda\")))\n",
    "print(tokenizer.decode(model.generate(**tokenizer(\"How are\", return_tensors=\"pt\").to(\"cuda\"), do_sample=False, max_new_tokens=20)[0]))\n",
    "print(tokenizer.decode(model.generate(**tokenizer(\"How are \", return_tensors=\"pt\").to(\"cuda\"), do_sample=False, max_new_tokens=20)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verbalization_venv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
