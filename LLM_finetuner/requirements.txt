# transformers
# transformers==4.40.0 # for llama-3
transformers>=4.40.0 # for llama-3
accelerate
peft
datasets
trl
bitsandbytes
evaluate
wandb

# module load cuda/12.1
wheel # required by flash-attn
flash-attn # pip install flash-attn --no-build-isolation

# constrained generation
outlines
pydantic
# git+https://github.com/outlines-dev/outlines.git pycountry pyairports

# for deployment
gradio

ipykernel
more_itertools

# for llama2 tokenizer
sentencepiece

# for "accelerate test"
pytest