# transformers
# transformers==4.40.0 # for llama-3
transformers>=4.40.0 # for llama-3
accelerate
peft
datasets
trl
bitsandbytes
evaluate
wandb

# wheel # required by flash-attn
# flash-attn # pip install flash-attn --no-build-isolation

# for deployment
gradio

ipykernel
more_itertools

# for llama2 tokenizer
sentencepiece

# for "accelerate test"
pytest