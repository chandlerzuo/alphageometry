{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T15:22:41.538065Z",
     "start_time": "2024-05-03T15:22:41.516983Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T15:24:38.629637Z",
     "start_time": "2024-05-03T15:24:37.813285Z"
    }
   },
   "source": [
    "##%%\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict\n",
    "import torch\n",
    "import more_itertools\n",
    "import logging\n",
    "import sys\n",
    "import textwrap\n",
    "import collections\n",
    "\n",
    "\n",
    "import tqdm\n",
    "from transformers import pipeline, HfArgumentParser\n",
    "from datasets import load_from_disk\n",
    "import gradio as gr\n",
    "from contextlib import nullcontext, redirect_stdout\n",
    "\n",
    "from LLM_finetuner.question_answer_utils import extract_answer, extract_question_prompt, get_question_answer_to_chat_formatter\n",
    "from LLM_finetuner.utils import load_model_for_inference, setup_logging, subset_dataset\n",
    "\n",
    "##%%\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "setup_logging()\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/03/2024 16:33:08 - WARNING - __main__ - Using dummy args\n"
     ]
    }
   ],
   "source": [
    "logger.warning(\"Using dummy args\")\n",
    "# model_name_or_path = \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/runs/verbalization/training/overfit_single_nocompl/gpt2\"\n",
    "# model_name_or_path = \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/runs/verbalization/training/overfit_single_nocompl/gpt2_2ex\"\n",
    "# model_name_or_path = \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/runs/verbalization/training/overfit_single_nocompl/gpt2_withpeft\"\n",
    "model_name_or_path = \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/cluster_runs/verbalization/training/failed_quote/gpt2_1000ex_peftFalse\"\n",
    "# model_name_or_path = \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/cluster_runs/verbalization/training/failed_quote/gpt2_1000ex_peftTrue\" # broken\n",
    "model_name_or_path = \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/cluster_runs/verbalization/training/failed_quote/Llama-2-7b-chat-hf_1000ex_peftTrue\"\n",
    "# model_name_or_path = \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/runs/verbalization/training/overfit_single_nocompl/gpt2_32ex_peftTrue_extratokens\"\n",
    "dataset_name = \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/runs/verbalization/datasets/alpha_geo_small_processed\"\n",
    "# dataset_test_name = \"test\"\n",
    "dataset_test_name = \"train\" # for overfitting exp\n",
    "filename_predictions_out = \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/runs/verbalization/predictions/exp_small/gpt2_predictions.txt\"\n",
    "# max_predict_samples = 2\n",
    "max_predict_samples = 2\n",
    "dataset_text_field = \"text\"\n",
    "max_new_tokens = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/03/2024 16:33:09 - INFO - __main__ - Generating predictions, writing to file '/home/mmordig/reinforcement/alphageometry/LLM_finetuner/runs/verbalization/predictions/exp_small/gpt2_predictions.txt'\n",
      "05/03/2024 16:33:09 - INFO - LLM_finetuner.utils - Loading model from '/home/mmordig/reinforcement/alphageometry/LLM_finetuner/cluster_runs/verbalization/training/failed_quote/Llama-2-7b-chat-hf_1000ex_peftTrue/checkpoint-3360'\n",
      "05/03/2024 16:33:10 - INFO - accelerate.utils.modeling - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.08s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 502.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m dataset \u001B[38;5;241m=\u001B[39m raw_datasets[dataset_test_name]\n\u001B[1;32m      5\u001B[0m dataset \u001B[38;5;241m=\u001B[39m subset_dataset(dataset, n_samples\u001B[38;5;241m=\u001B[39mmax_predict_samples)\n\u001B[0;32m----> 7\u001B[0m model, tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mload_model_for_inference\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name_or_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m is_chat_model \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mchat_template \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_chat_model:\n",
      "File \u001B[0;32m~/reinforcement/alphageometry/LLM_finetuner/utils.py:44\u001B[0m, in \u001B[0;36mload_model_for_inference\u001B[0;34m(model_checkpoints_dir)\u001B[0m\n\u001B[1;32m     39\u001B[0m bnb_config \u001B[38;5;241m=\u001B[39m BitsAndBytesConfig(load_in_4bit\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, bnb_4bit_compute_dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mbfloat16)\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# bnb_config = None\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# fails when vocab size changes\u001B[39;00m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# https://discuss.huggingface.co/t/loading-peft-model-from-checkpoint-leading-into-size-missmatch/71944\u001B[39;00m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;66;03m# model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\", quantization_config=bnb_config)\u001B[39;00m\n\u001B[0;32m---> 44\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoPeftModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mauto\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquantization_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbnb_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     45\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name_or_path) \u001B[38;5;66;03m# don't add eos_token since in generation mode\u001B[39;00m\n\u001B[1;32m     46\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39mpadding_side \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;66;03m# for auto-regressive generation\u001B[39;00m\n",
      "File \u001B[0;32m~/reinforcement/alphageometry/LLM_finetuner/verbalization_venv3/lib/python3.10/site-packages/peft/auto.py:128\u001B[0m, in \u001B[0;36m_BaseAutoPeftModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, adapter_name, is_trainable, config, **kwargs)\u001B[0m\n\u001B[1;32m    123\u001B[0m     tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m    124\u001B[0m         pretrained_model_name_or_path, trust_remote_code\u001B[38;5;241m=\u001B[39mkwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrust_remote_code\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    125\u001B[0m     )\n\u001B[1;32m    126\u001B[0m     base_model\u001B[38;5;241m.\u001B[39mresize_token_embeddings(\u001B[38;5;28mlen\u001B[39m(tokenizer))\n\u001B[0;32m--> 128\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_target_peft_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    129\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbase_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    130\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    131\u001B[0m \u001B[43m    \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madapter_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    132\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_trainable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_trainable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    133\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    134\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    135\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/reinforcement/alphageometry/LLM_finetuner/verbalization_venv3/lib/python3.10/site-packages/peft/peft_model.py:356\u001B[0m, in \u001B[0;36mPeftModel.from_pretrained\u001B[0;34m(cls, model, model_id, adapter_name, is_trainable, config, **kwargs)\u001B[0m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    355\u001B[0m     model \u001B[38;5;241m=\u001B[39m MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config\u001B[38;5;241m.\u001B[39mtask_type](model, config, adapter_name)\n\u001B[0;32m--> 356\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_adapter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_trainable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_trainable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    357\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[0;32m~/reinforcement/alphageometry/LLM_finetuner/verbalization_venv3/lib/python3.10/site-packages/peft/peft_model.py:727\u001B[0m, in \u001B[0;36mPeftModel.load_adapter\u001B[0;34m(self, model_id, adapter_name, is_trainable, **kwargs)\u001B[0m\n\u001B[1;32m    724\u001B[0m         peft_config\u001B[38;5;241m.\u001B[39minference_mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m is_trainable\n\u001B[1;32m    725\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_adapter(adapter_name, peft_config)\n\u001B[0;32m--> 727\u001B[0m adapters_weights \u001B[38;5;241m=\u001B[39m \u001B[43mload_peft_weights\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch_device\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhf_hub_download_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    729\u001B[0m \u001B[38;5;66;03m# load the weights into the model\u001B[39;00m\n\u001B[1;32m    730\u001B[0m load_result \u001B[38;5;241m=\u001B[39m set_peft_model_state_dict(\u001B[38;5;28mself\u001B[39m, adapters_weights, adapter_name\u001B[38;5;241m=\u001B[39madapter_name)\n",
      "File \u001B[0;32m~/reinforcement/alphageometry/LLM_finetuner/verbalization_venv3/lib/python3.10/site-packages/peft/utils/save_and_load.py:326\u001B[0m, in \u001B[0;36mload_peft_weights\u001B[0;34m(model_id, device, **hf_hub_download_kwargs)\u001B[0m\n\u001B[1;32m    324\u001B[0m         adapters_weights \u001B[38;5;241m=\u001B[39m safe_load_file(filename, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    325\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 326\u001B[0m         adapters_weights \u001B[38;5;241m=\u001B[39m \u001B[43msafe_load_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    327\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    328\u001B[0m     adapters_weights \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(filename, map_location\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(device))\n",
      "File \u001B[0;32m~/reinforcement/alphageometry/LLM_finetuner/verbalization_venv3/lib/python3.10/site-packages/safetensors/torch.py:313\u001B[0m, in \u001B[0;36mload_file\u001B[0;34m(filename, device)\u001B[0m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m safe_open(filename, framework\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m, device\u001B[38;5;241m=\u001B[39mdevice) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m    312\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m f\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m--> 313\u001B[0m         result[k] \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mk\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    314\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 502.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "logger.info(f\"Generating predictions, writing to file '{filename_predictions_out}'\")\n",
    "\n",
    "raw_datasets = load_from_disk(dataset_name)\n",
    "dataset = raw_datasets[dataset_test_name]\n",
    "dataset = subset_dataset(dataset, n_samples=max_predict_samples)\n",
    "\n",
    "model, tokenizer = load_model_for_inference(model_name_or_path)\n",
    "\n",
    "is_chat_model = tokenizer.chat_template is not None\n",
    "if is_chat_model:\n",
    "    logger.info(\"Detected chat model, formatting according to chat template\")\n",
    "    # assumes user-assistant roles\n",
    "    prompt_extraction_function = get_question_answer_to_chat_formatter(tokenizer, text_column=None, add_generation_prompt=True)\n",
    "else:\n",
    "    prompt_extraction_function = extract_question_prompt\n",
    "    \n",
    "def extract_extra_cols(batch):\n",
    "    return {\n",
    "        \"question_prompt\": [prompt_extraction_function(item) for item in batch[dataset_text_field]],\n",
    "        \"answer_only\": [extract_answer(item) for item in batch[dataset_text_field]],\n",
    "    }\n",
    "dataset = dataset.map(extract_extra_cols, batched=True)\n",
    "logger.info(f\"Example datapoint: {dataset[0]}\")\n",
    "\n",
    "# use_cache to avoid recomputing hidden states, see https://discuss.huggingface.co/t/what-is-the-purpose-of-use-cache-in-decoder/958\n",
    "# max_new_tokens = 70\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, \n",
    "    num_return_sequences=2, num_beams=4, do_sample=True, max_new_tokens=max_new_tokens, use_cache=True,\n",
    "    return_full_text=False, # answer only\n",
    "    num_workers=2, \n",
    "    # batch_size=2 # triggers a cuda device-side error, maybe related to https://github.com/huggingface/transformers/issues/22546\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# model_name_or_path = \"gpt2\"\n",
    "# model_name_or_path = \"meta-llama/Llama-2-7b-hf\"\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False, add_eos_token=True, pad_token=\"[PAD]\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True, add_eos_token=True)\n",
    "# tokenizer.pad_token = \"[PAD]\"\n",
    "# print(tokenizer.eos_token)\n",
    "# print(tokenizer.pad_token)\n",
    "\n",
    "# # ?tokenizer\n",
    "# tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_prompt = dataset[0][\"question_prompt\"]\n",
    "# text = dataset[0][\"text\"]\n",
    "# tokenizer.decode(tokenizer(text)[\"input_ids\"], include_special_tokens=True)[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 804.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "Query: \n",
      "### Question: Points B, A, C are defined such that triangle ABC is an⏎\n",
      "equilateral triangle. Define points D, F, & E such that E, D, and F is⏎\n",
      "a right angle isosceles triangle with the right angle at D. Point G is⏎\n",
      "defined such that G, A, E are three consecutive vertices of a square.⏎\n",
      "Circle centered at C with radius CE intersects circle centered at G⏎\n",
      "with radius GE at H and E. Points I and J are defined such that line⏎\n",
      "IC and line JC are the two tangents to circle centered at B with⏎\n",
      "radius BF at point I and J respectively.. Define point K such that⏎\n",
      "line DI and line AG are parallel. line JK perpendicular to line DI.⏎\n",
      "line EK perpendicular to line AG. line JK meets line EK at the point⏎\n",
      "K. ### Answer:\n",
      "Expected answer: \n",
      "A B C = ieq_triangle A B C; D E F = risos D E F; G = psquare G A E; H⏎\n",
      "= intersection_cc H C G E; I J = tangent I J C B F; K =⏎\n",
      "intersection_tt K J D I E A G\n",
      "Number of candidates that are equal to expected: 0\n",
      "Number of candidates that begin with expected: 0\n",
      "#################### Candidate 1 (appears 1 times) ####################\n",
      "A B C = ieq_triangle A B C; D E F = r_triangle D E F; G = r_triangle G⏎\n",
      "A E; H = intersection_cc H E B; I J = tangent I J C B F; K =⏎\n",
      "intersection_j K J E F B; L = intersection_ <MAX token length exceeded>\n",
      "#################### Candidate 2 (appears 1 times) ####################\n",
      "A B C = ieq_triangle A B C; D E F = r_triangle D E F; G = r_triangle G⏎\n",
      "A E; H = intersection_cc H E B; I J = tangent I J C B F; K =⏎\n",
      "intersection_j K J E F A G B; L = <MAX token length exceeded>\n",
      "################################################################################\n",
      "Query: \n",
      "### Question: Points A, D, B, C are defined such that A, B, C, and D⏎\n",
      "is a trapezoid where line AD is equal to line BC. E is a point. F is⏎\n",
      "defined such that line FC is touching to circle centered at A with⏎\n",
      "radius AC at the point F. Define points G, J, I, & H such that J is⏎\n",
      "located at the incenter of triangle ECB with touchpoints H, I, and G.⏎\n",
      "K is defined such that K is a point such that ∠FDJ = ∠JDK. ### Answer:\n",
      "Expected answer: \n",
      "A B C D = eq_trapezoid A B C D; E = free E; F = lc_tangent F C A; G H⏎\n",
      "I J = incenter2 G H I J E C B; K = angle_mirror K F D J\n",
      "Number of candidates that are equal to expected: 0\n",
      "Number of candidates that begin with expected: 0\n",
      "#################### Candidate 1 (appears 1 times) ####################\n",
      "A B C D = eq_trapezoid A B C D; E = r_triangle E A B; F = lc_tangent F⏎\n",
      "C; G H I J = incenter4 G H I J C; K = on_dia K E B; L = on_dia K E B; <MAX token length exceeded>\n",
      "#################### Candidate 2 (appears 1 times) ####################\n",
      "A B C D = eq_trapezoid A B C D; E = r_triangle E A B; F = lc_tangent F⏎\n",
      "C; G H I J = incenter4 G H I J C; K = on_dia K E B; L = on_dia K E D J <MAX token length exceeded>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "break_nicely = lambda x: \"\\u23CE\\n\".join(textwrap.wrap(x)) # symbol \"⏎\" for line breaks\n",
    "# using a pipe/dataset is faster because GPU works in the background while writing to file\n",
    "# with open(filename_predictions_out, 'w') as f, redirect_stdout(f):\n",
    "# with nullcontext():\n",
    "for (out, question_prompt, gt_answer) in tqdm.tqdm(zip(pipe(dataset[\"question_prompt\"]), dataset[\"question_prompt\"], dataset[\"answer_only\"])):\n",
    "    print(\"#\"*80)\n",
    "    print(\"Query: \")\n",
    "    print(break_nicely(question_prompt))\n",
    "    print(\"Expected answer: \")\n",
    "    print(break_nicely(gt_answer))\n",
    "    # strips whitespace because generated text has leading and trailing whitespace\n",
    "    out_counted = collections.Counter([candidate[\"generated_text\"].strip() for candidate in out])\n",
    "    gt_answer = gt_answer.strip()\n",
    "    print(f\"Number of candidates that are equal to expected: {out_counted.get(gt_answer, 0)}\")\n",
    "    print(f\"Number of candidates that begin with expected:\", sum(out_counted[key] for key in out_counted if key.startswith(gt_answer)))\n",
    "    # for (i, candidate) in enumerate(out):\n",
    "    #     candidate_text = candidate[\"generated_text\"]\n",
    "    for (i, (candidate_text, count)) in enumerate(out_counted.items()):\n",
    "        # logger.info(f\"Generated text: {candidate_text}\")\n",
    "        # answer = extract_answer(candidate_text)\n",
    "        answer = candidate_text\n",
    "        print(\"#\"*20 + f\" Candidate {i+1} (appears {count} times) \" + \"#\"*20)\n",
    "        extra = \"\"\n",
    "        # not perfect because tokenizing with question_prompt may lead to different tokenization\n",
    "        if len(tokenizer(answer)[\"input_ids\"]) == max_new_tokens:\n",
    "            extra = \" <MAX token length exceeded>\"\n",
    "        print(break_nicely(answer) + extra)\n",
    "    # sys.stdout.flush()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(out_counted.keys())[0])\n",
    "print(' ' + gt_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(out_counted.keys())[0] == (' ' + gt_answer)\n",
    "def show_diff(s1, s2):\n",
    "    for (c1, c2) in zip(s1, s2):\n",
    "        if c1 != c2:\n",
    "            print(c1, c2)\n",
    "show_diff(list(out_counted.keys())[0], (' ' + gt_answer))\n",
    "\n",
    "s1 = list(out_counted.keys())[0]\n",
    "s2 = (' ' + gt_answer)\n",
    "s1[len(s2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[0][\"text\"]\n",
    "question_prompt = \"### Question: Points B, A, C are defined such that triangle ABC is an equilateral triangle. Define points D, F, & E such that E, D, and F is a right angle isosceles triangle with the right angle at D. Point G is defined such that G, A, E are three consecutive vertices of a square. Circle centered at C with radius CE intersects circle centered at G with radius GE at H and E. Points I and J are defined such that line IC and line JC are the two tangents to circle centered at B with radius BF at point I and J respectively.. Define point K such that line DI and line AG are parallel. line JK perpendicular to line DI. line EK perpendicular to line AG. line JK meets line EK at the point K. ### Answer:\"\n",
    "print(tokenizer(question_prompt + \" \")[\"input_ids\"][-10:] + tokenizer(\"answer\")[\"input_ids\"])\n",
    "print(tokenizer(question_prompt + \" answer\")[\"input_ids\"][-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset_text_field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer(\"Hello answer\")[\"input_ids\"])\n",
    "print(tokenizer(\"Hello answer \")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.generate(**tokenizer(\"Hello answer\", return_tensors=\"pt\").to(\"cuda\")))\n",
    "print(tokenizer.decode(model.generate(**tokenizer(\"How are\", return_tensors=\"pt\").to(\"cuda\"), do_sample=False, max_new_tokens=20)[0]))\n",
    "print(tokenizer.decode(model.generate(**tokenizer(\"How are \", return_tensors=\"pt\").to(\"cuda\"), do_sample=False, max_new_tokens=20)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verbalization_venv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
